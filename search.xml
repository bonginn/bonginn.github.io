<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>WQS 二分搜</title>
    <url>/2025/01/08/WQS%E4%BA%8C%E5%88%86%E6%90%9C/</url>
    <content><![CDATA[<p>一篇WQS二分搜的筆記，希望能讓自己不要這麼快忘記的個優化技巧。<br><span id="more"></span></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>WQS 二分搜又稱Aliens優化，是一個可以將二維的DP問題轉化成一個維度的優化技巧<br>複雜度可以從$O(N^2)$下降到$O(NlogC)$，但題目本身需要一些特性。<br>已知某函數$f(x)$為concave function，且我們有某種演算法可以在線性時間內求出$f(x) - px$的最大值及最大值<br>發生的位置($x_0$)。<br>接下來，我們畫出$p=0 - 5$的Cases :<br><img src="/images/Chart.png" alt="Credit : 4o"><br>可以發現，$x_0$隨著$p$上升逐漸下降！<br>因此若我們希望找到$f(x)$的最大值且$x&lt;=k$，我們可以對$p$做二分搜，找到最近的$p$使得$f(x) - px$的$x_0$$\le k$，最後答案就會是演算法的輸出$+ pk$。</p>
<h2 id="Informal-Proof"><a href="#Informal-Proof" class="headerlink" title="Informal Proof"></a>Informal Proof</h2><p>至於為什麼當p越大，極值發生位置會越來越前面，簡單的證明如下：<br>當$f(x)$極值發生時的必要條件是$\nabla f(x) = 0$<br>$\nabla (f(x) - px) = \nabla f(x) - \nabla px = \nabla f(x) - p = 0$<br>已知$f(x)$是concave，因此 $\nabla f(x)$ 遞減<br>因此當$p \uparrow$，$x$必須越來越小。<br>然而，WQS真正困難的部分其實是證明$f(x)$是concave的，很多時候看到題目都會靠直覺猜它是concave，然後直接使用此性質（就像猜某個greedy方式會是最佳解）。</p>
<h2 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h2><p>我覺得WQS二分搜有一個很好的intuition：<br>$f(x)$是某件事做了$x$次後得到的值，$p$就可以當作每做一次所需的成本，當成本($p$)越大，我們就不能做太多次操作（若我們的目標是maximize $f(x)$）。<br>因此！理所當然當$p$越大，$x_0$就會越小。</p>
<h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><p>直接來看一題很經典的WQS二分的題目<a href="https://tioj.ck.tp.edu.tw/problems/2039">AI-666 賺多少</a>：<br>題目敘述：給定$n$天股票價格，求只能交易$k$次的情況下最多能賺多少錢？（假設最多同時只能持有一張股票）<br>這題有一個很簡單的dp解：<br>$dp0[i][j]$表示在第$i$天，交易了$j$次且當下沒有股票的最佳解<br>$dp1[i][j]$表示在第$i$天，交易了$j$次且當下持有股票的最佳解<br>轉移式：<br>$dp0[i][j] = max(dp0[i-1][j], dp1[i-1][j-1] + a[i])$<br>$dp1[i][j] = max(dp1[i-1][j], dp0[i-1][j] - a[i])$<br>最後答案就是$dp0[n][k]$。<br>但很明顯，此做法的時間、空間複雜度為$O(n^2)$。<br>嘗試將問題轉換成「不限制交易次數」，我們可以發現轉移式就會變成：<br>$dp0[i] = max(dp0[i-1], dp1[i-1] + a[i])$<br>$dp1[i] = max(dp1[i-1], dp0[i-1] - a[i])$<br>其中，$dp0[i]$表示在第$i$天，當下沒有股票的最佳解，$dp1[i]$表示在第$i$天，當下持有股票的最佳解。<br>接著，我們嘗試將每次交易都增加一個$p$元的手續費，此時轉移式變成：<br>$dp0[i] = max(dp0[i-1], dp1[i-1] + a[i])$<br>$dp1[i] = max(dp1[i-1], dp0[i-1] - a[i] - p)$<br>我們可以在$O(n)$的時間解決這個「有手續費且沒有交易次數限制」的問題。<br>最後，我們可以對$p$做二分搜找到某個$p$，使得發生最大值時的交易次數$\leq k$<br>最終答案就會是此演算法的輸出$+ pk$。 </p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> int long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ll long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pb push_back</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> sz(x) (x).size()</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> all(x) (x).begin(), (x).end()</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> fastio cin.tie(0); ios_base::sync_with_stdio(false);</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">2e6</span><span class="number">+5</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> mod = <span class="number">1e9</span><span class="number">+7</span>; <span class="comment">// 998244353;</span></span><br><span class="line"><span class="type">const</span> ll inf = <span class="number">1ll</span> &lt;&lt; <span class="number">61</span>;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>,<span class="type">int</span>&gt; P;</span><br><span class="line"><span class="type">int</span> n, k;</span><br><span class="line"><span class="type">int</span> a[maxn];</span><br><span class="line">P dp0[maxn], dp1[maxn];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">()</span></span>&#123;</span><br><span class="line">    dp0[<span class="number">0</span>] = <span class="built_in">P</span>(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    dp1[<span class="number">0</span>] = <span class="built_in">P</span>(-inf, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">P <span class="title">cal</span><span class="params">(<span class="type">int</span> p)</span></span>&#123;</span><br><span class="line">    <span class="built_in">init</span>();</span><br><span class="line">    P res = &#123;<span class="number">0</span>, inf&#125;;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)&#123;</span><br><span class="line">        <span class="comment">// 沒股票</span></span><br><span class="line">        <span class="keyword">if</span>(dp0[i<span class="number">-1</span>] &gt; <span class="built_in">P</span>(dp1[i<span class="number">-1</span>].first + a[i] - p, dp1[i<span class="number">-1</span>].second - <span class="number">1</span>))</span><br><span class="line">            dp0[i] = dp0[i<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            dp0[i] = <span class="built_in">P</span>(dp1[i<span class="number">-1</span>].first + a[i] - p, dp1[i<span class="number">-1</span>].second - <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 有股票</span></span><br><span class="line">        <span class="keyword">if</span>(dp1[i<span class="number">-1</span>] &gt; <span class="built_in">P</span>(dp0[i<span class="number">-1</span>].first - a[i], dp0[i<span class="number">-1</span>].second))</span><br><span class="line">            dp1[i] = dp1[i<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            dp1[i] = <span class="built_in">P</span>(dp0[i<span class="number">-1</span>].first - a[i], dp0[i<span class="number">-1</span>].second);</span><br><span class="line">        res = <span class="built_in">max</span>(res, dp0[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span>&#123;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; k;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)</span><br><span class="line">        cin &gt;&gt; a[i];</span><br><span class="line">    <span class="type">int</span> l = <span class="number">0</span>, r = <span class="number">5e4</span>;</span><br><span class="line">    P res = <span class="built_in">cal</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">int</span> pp;</span><br><span class="line">    <span class="keyword">if</span>(-res.second &lt;= k)&#123;</span><br><span class="line">        cout &lt;&lt; res.first;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(l &lt;= r)&#123;</span><br><span class="line">        <span class="type">int</span> mid = (l + r) / <span class="number">2</span>;</span><br><span class="line">        res = <span class="built_in">cal</span>(mid);</span><br><span class="line">        <span class="keyword">if</span>(-res.second &gt; k)&#123;</span><br><span class="line">            pp = mid;</span><br><span class="line">            l = mid + <span class="number">1</span>;</span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            r = mid - <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">cal</span>(pp<span class="number">+1</span>).first + (pp<span class="number">+1</span>) * k;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">signed</span> <span class="title">main</span><span class="params">()</span></span>&#123;fastio</span><br><span class="line">    <span class="type">int</span> T = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">//cin &gt;&gt; T;</span></span><br><span class="line">    <span class="keyword">while</span>(T--)&#123;</span><br><span class="line">        <span class="built_in">solve</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在TIOJ上要稍微壓一點常數（e.g. 二分搜範圍）</p>
<p>WQA二分搜的過程其實有不少細節，請未來的我自行體會。<br>但就結論而言，一個簡單且可以避免錯誤的方法是「找最大的$p$使得最後的交易次數不滿足題目限制」<br>最後我們所需$p$就會是$p+1$(如果是離散的話)。 </p>
<h2 id="Related-Problems"><a href="#Related-Problems" class="headerlink" title="Related Problems"></a>Related Problems</h2><p><a href="https://zerojudge.tw/ShowProblem?problemid=h926"> 美食博覽會 (k 值加大版)</a><br><a href="https://cses.fi/problemset/task/2086">Subarray Squares</a><br><a href="https://www.luogu.com.cn/problem/P2619">Tree I</a><br><a href="https://codeforces.com/contest/958/problem/E2">E2. Guard Duty (medium)</a><br><a href="https://codeforces.com/contest/1279/problem/F">F. New Year and Handle Change</a><br><a href="https://codeforces.com/contest/739/problem/E">E. Gosha is hunting</a> </p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>Backpropagation 反向傳播</title>
    <url>/2025/02/17/backpropagation/</url>
    <content><![CDATA[<p>學PyTorch不如自己刻一次Backpropagation。<br><span id="more"></span></p>
<h1 id="Opening"><a href="#Opening" class="headerlink" title="Opening"></a><strong>Opening</strong></h1><p>本篇文章是建立在對Gradient Descent、Neural Network架構有基礎了解的前提下，介紹如何透過Backpropagation來更新神經網路的參數，並且使用NumPy手刻出Backpropagation的過程。</p>
<h1 id="Forward-向前傳播"><a href="#Forward-向前傳播" class="headerlink" title="Forward 向前傳播"></a><strong>Forward 向前傳播</strong></h1><p>首先，先來看神經網路的基本架構：</p>
<p><p align="center">
  <img src="/images/nn.png" alt="Credit: 4o">
</p><br>這是一個簡易的神經網路，輸入、隱藏層皆有兩個Neurons加上一個Bias，輸出則有一個Neurons。對於某Neuron $X_i$，向前傳播過程可以寫成：</p>
<div>
$$
\begin{aligned}
Z^{(1)}_j &= \sum_{i=1}^{D} W^{(1)}_{ij}X_i + B^{(1)}_j \\
A^{(1)}_j &= h(Z^{(1)}_j) \\
Z^{(2)}_k &= \sum_{j=1}^{M} W^{(2)}_{jk}A^{(1)}_j + B^{(2)}_k \\
A^{(2)}_k &= Z^{(2)}_k \\

\text{E}(W,B) &= \frac{1}{2 } \sum_{k=1}^{K} (A^{(2)}_k - Y_k)^2 
\end{aligned}
$$
</div>
這裡的$h$表示隱藏層的激活函數（Activation Function），這裡因為以Regression為例，所以輸出層的激活函數為Identity Function。$\text{E}$表示Loss Function，這裡以Mean Squared Error為例。

若僅考慮Variables/Activations的Dependency，可以寫成：
<div>
$$
\begin{aligned}
X_i \rightarrow W^{(1)}_{ij} \rightarrow Z^{(1)}_j \rightarrow A^{(1)}_j \rightarrow W^{(2)}_{jk} \rightarrow Z^{(2)}_k \rightarrow A^{(2)}_k \leftrightarrow Y_k \rightarrow \text{E}
\end{aligned}
$$
</div>

<h1 id="Backward-反向傳播"><a href="#Backward-反向傳播" class="headerlink" title="Backward 反向傳播"></a><strong>Backward 反向傳播</strong></h1><p>接著，我們要來更新$W^{(2)}$的參數，我們的目標是計算$\frac{\partial E}{\partial W^{(2)}_{jk}}$，但這個東西很難直接算出來，因此我們考慮使用<strong>Chain Rule</strong>，透過上式的Dependency關係將其拆成 :</p>
<div>
$$
\begin{aligned}
\frac{\partial E}{\partial W^{(2)}_{jk}} &= \frac{\partial E}{\partial Z^{(2)}_k} \cdot \frac{\partial Z^{(2)}_k}{\partial W^{(2)}_{jk}} \\
\end{aligned}
$$  
</div>

<p>透過一些簡單的偏微分計算，我們得到：</p>
<div>
$$
\begin{aligned}
\frac{\partial E}{\partial Z^{(2)}_k} &= A^{(2)}_k - Y_k = \delta_{k} \\
\frac{\partial Z^{(2)}_k}{\partial W^{(2)}_{jk}} &= A^{(1)}_j
\end{aligned}
$$
</div>
接著再次使用Chain Rule，分解$\frac{\partial E}{\partial W^{(1)}_{ij}}$：
<div>
$$
\begin{aligned}
\frac{\partial E}{\partial W^{(1)}_{ij}} &= 
\sum_{k=1}^{K} \left(
\frac{\partial E}{\partial Z^{(2)}_k} \cdot
\frac{\partial Z^{(2)}_k}{\partial Z^{(1)}_j} \cdot
\frac{\partial Z^{(1)}_j}{\partial W^{(1)}_{ij}} \right) \\
\end{aligned}
$$
</div>
再複習一次大一微積分：
<div>
$$
\begin{aligned}
\frac{\partial E}{\partial Z^{(2)}_k} &= \delta_{k} \\
\frac{\partial Z^{(2)}_k}{\partial Z^{(1)}_j} &= W^{(2)}_{jk} \cdot h^{'}(Z^{(1)}_j) \\
\frac{\partial Z^{(1)}_j}{\partial W^{(1)}_{ij}} &= X_i
\end{aligned}
$$
</div>

<p>其中，我們定義第一層的誤差信號 $\delta_j$ 為：</p>
<div>
$$
\begin{aligned}
\delta_j &= \frac{\partial E}{\partial Z^{(1)}_j} = \sum_{k=1}^{K} W^{(2)}_{jk} \delta_k h'(Z^{(1)}_j)
\end{aligned}
$$
</div>

<p>將上述幾式整理，我們可以得到：</p>
<div>
$$
\begin{aligned}
\frac{\partial E}{\partial W^{(2)}_{jk}} &= \delta_{k} \cdot A^{(1)}_j \\
\frac{\partial E}{\partial W^{(1)}_{ij}} &= \delta_{j} \cdot X_i
\end{aligned}
$$
</div>

<p>對於Bias的Gradient，其實就是：</p>
<div>
$$
\begin{aligned}
\frac{\partial E}{\partial B^{(2)}_{k}} &= \delta_{k} \\
\frac{\partial E}{\partial B^{(1)}_{j}} &= \delta_{j}
\end{aligned}
$$
</div>

<p>如此一來我們就算完最複雜的部分了，可以進入實作了！</p>
<h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h1><p>在開始實作前，我們先釐清各個矩陣的維度：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>符號</th>
<th>說明</th>
<th>維度</th>
</tr>
</thead>
<tbody>
<tr>
<td>$X$</td>
<td>輸入矩陣</td>
<td>(m, n_input)</td>
</tr>
<tr>
<td>$W^{(1)}$</td>
<td>第一層的權重矩陣</td>
<td>(n_input, n_hidden)</td>
</tr>
<tr>
<td>$B^{(1)}$</td>
<td>第一層的偏差矩陣</td>
<td>(1, n_hidden)</td>
</tr>
<tr>
<td>$Z^{(1)}$</td>
<td>第一層的線性組合</td>
<td>(m, n_hidden)</td>
</tr>
<tr>
<td>$A^{(1)}$</td>
<td>第一層的激活函數結果</td>
<td>(m, n_hidden)</td>
</tr>
<tr>
<td>$W^{(2)}$</td>
<td>第二層的權重矩陣</td>
<td>(n_hidden, n_output)</td>
</tr>
<tr>
<td>$B^{(2)}$</td>
<td>第二層的偏差矩陣</td>
<td>(1, n_output)</td>
</tr>
<tr>
<td>$Z^{(2)}$</td>
<td>第二層的線性組合</td>
<td>(m, n_output)</td>
</tr>
<tr>
<td>$A^{(2)}$</td>
<td>第二層的激活函數結果</td>
<td>(m, n_output)</td>
</tr>
</tbody>
</table>
</div>
<p>其中，m為Batch Size，n_input為輸入的特徵數，n_hidden為隱藏層的Neuron數，n_output為輸出的Neuron數。</p>
<h2 id="Forward"><a href="#Forward" class="headerlink" title="Forward"></a><strong>Forward</strong></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        x (numpy.ndarray): Input matrix of shape (batch_size, n_input).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        numpy.ndarray: Output matrix of shape (batch_size, n_output).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="variable language_">self</span>.z1 = np.matmul(x, <span class="variable language_">self</span>.w1) + <span class="variable language_">self</span>.b1</span><br><span class="line">    <span class="variable language_">self</span>.a1 = <span class="variable language_">self</span>.ReLU(<span class="variable language_">self</span>.z1)</span><br><span class="line">    <span class="variable language_">self</span>.z2 = np.matmul(<span class="variable language_">self</span>.a1, <span class="variable language_">self</span>.w2) + <span class="variable language_">self</span>.b2</span><br><span class="line">    <span class="variable language_">self</span>.a2 = <span class="variable language_">self</span>.z2</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.a2</span><br></pre></td></tr></table></figure>
<p>在這裡選用ReLU作為隱藏層的Activation Function。</p>
<h2 id="Backward"><a href="#Backward" class="headerlink" title="Backward"></a><strong>Backward</strong></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, X, y, pred, lr</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        X (numpy.ndarray): Input matrix of shape (batch_size, n_input).</span></span><br><span class="line"><span class="string">        y (numpy.ndarray): Target matrix of shape (batch_size, n_output).</span></span><br><span class="line"><span class="string">        pred (numpy.ndarray): Output matrix of shape (batch_size, n_output).</span></span><br><span class="line"><span class="string">        lr (float): Learning rate.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># STEP 1</span></span><br><span class="line">    batch_size = X.shape[<span class="number">0</span>]</span><br><span class="line">    delta_output = <span class="number">2</span> * (pred - y) / batch_size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># STEP 2</span></span><br><span class="line">    dw2 = np.matmul(<span class="variable language_">self</span>.a1.T, delta_output)</span><br><span class="line">    db2 = np.<span class="built_in">sum</span>(delta_output, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># STEP 3</span></span><br><span class="line">    delta_hidden = np.matmul(delta_output, <span class="variable language_">self</span>.w2.T) * <span class="variable language_">self</span>.ReLU_derivative(<span class="variable language_">self</span>.z1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># STEP 4</span></span><br><span class="line">    dw1 = np.matmul(X.T, delta_hidden)</span><br><span class="line">    db1 = np.<span class="built_in">sum</span>(delta_hidden, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># UPDATE</span></span><br><span class="line">    <span class="variable language_">self</span>.w1 -= lr * dw1</span><br><span class="line">    <span class="variable language_">self</span>.b1 -= lr * db1</span><br><span class="line">    <span class="variable language_">self</span>.w2 -= lr * dw2</span><br><span class="line">    <span class="variable language_">self</span>.b2 -= lr * db2</span><br></pre></td></tr></table></figure>
<p>這裡的ReLU_derivative是ReLU的導函數（也可以換成其他Activation Function，但記得要differentiable）。<br>$ \delta_k  \text{加了} \frac{2}{\text{batch_size}}$ 常數是因為真正MSE函式應為 <code>np.mean((A - Y) ** 2)</code>，這個mean是指將$\textbf{所有資料}$的Square Error取平均，而上面的MSE公式則是為了讓$\textbf{單一資料}$Gradient計算時會比較漂亮，所以乘了一個$\frac{1}{2}$。<br>當初在寫Backward時一直不懂矩陣到底要怎麼乘，但後來發現只要想清楚上面的公式，再對照好矩陣維度就可以了！</p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a><strong>Training</strong></h2><p>再來來寫一個簡單的Training Function：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y, epochs=<span class="number">1000</span>, lr=<span class="number">0.01</span>, verbose=<span class="number">10</span></span>):</span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   Parameters:</span></span><br><span class="line"><span class="string">       X (numpy.ndarray): Input matrix of shape (batch_size, n_input).</span></span><br><span class="line"><span class="string">       y (numpy.ndarray): Target matrix of shape (batch_size, n_output).</span></span><br><span class="line"><span class="string">       epochs (int): Number of epochs.</span></span><br><span class="line"><span class="string">       lr (float): Learning rate.</span></span><br><span class="line"><span class="string">       verbose (int): Frequency of printing the loss</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">       pred = <span class="variable language_">self</span>.forward(X)</span><br><span class="line">       loss = <span class="variable language_">self</span>.MSE(y, pred)</span><br><span class="line">       <span class="variable language_">self</span>.backward(X, y, pred, lr)</span><br><span class="line">       <span class="keyword">if</span> epoch % verbose == <span class="number">0</span>:</span><br><span class="line">           <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.6</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h1><p>這裡我以Regression為例，但實際上只要將Loss Function、Activation Function換掉，就可以應用在其他任務上。以下是一些常見的任務：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>Tasks</strong></th>
<th><strong>輸出層 Activation Function</strong></th>
<th><strong>Loss Function</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>回歸（Regression）</strong></td>
<td><code>Identity Function</code></td>
<td><strong>Mean Squared Error</strong></td>
</tr>
<tr>
<td><strong>二元分類（Binary Classification）</strong></td>
<td><code>Sigmoid</code></td>
<td><strong>Binary Cross-Entropy</strong></td>
</tr>
<tr>
<td><strong>多類分類（Multi-Class Classification）</strong></td>
<td><code>Softmax</code></td>
<td><strong>Categorical Cross-Entropy</strong></td>
</tr>
</tbody>
</table>
</div>
<p>再做一些微分時小小的修改就可以了。但我認為重點在於理解Backpropagation中使用Chain Rule的技巧，還有計算Partial Derivative的一些細節。</p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>code</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Roofline Model for Performance Analysis</title>
    <url>/2025/03/09/roofline/</url>
    <content><![CDATA[<p>A note about Roofline Model.<br><span id="more"></span></p>
<h1 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a><strong>Definition</strong></h1><ol>
<li><strong>FLOPs</strong> : Number of Floating Point Operations. A Multiply-Add operation(MAC) is counted as 2 FLOPs.</li>
<li><p><strong>FLOPS</strong> : Floating Point Operations Per Second.</p>
<div>
$$
\begin{aligned}
\text{FLOPS} &= \frac{\text{FLOPs}}{\text{Second}} \\
\end{aligned}
$$
</div>
</li>
<li><p><strong>OPs</strong> : Number of Operations. (Not necessarily Floating Point Operations)</p>
</li>
<li><strong>OPS</strong> : Number of Operations Per Second.<div>
$$
\begin{aligned}
\text{OPS} &= \frac{\text{OPs}}{\text{Second}} \\
\end{aligned}
$$
</div></li>
<li><strong>Memory Bandwidth</strong> : The rate at which data can be read from or written to memory (Bytes per second).</li>
<li><strong>Arithmetic Intensity</strong> : The ratio of Total OPs performed to the Total Bytes moved.<div>
$$
\begin{aligned}
\text{I} &= \frac{C \text{(Computations, OPs)}}{M \text{(Memory Access, Bytes)}} \\
\end{aligned}
$$
</div>


</li>
</ol>
<h1 id="Process-Of-DNN-Inference-on-Hardware"><a href="#Process-Of-DNN-Inference-on-Hardware" class="headerlink" title="Process Of DNN Inference on Hardware"></a><strong>Process Of DNN Inference on Hardware</strong></h1><p>Step 1 (Memory Access): Moving Input/Weghts from memory to CPU/GPU.<br>Step 2 (Compute): Perform the operations(e.g. Linear, Convolution…).<br>Step 3 (Memory Access): Writing the computed activation back to memory.<br>The process can be visualized as below:</p>
<p><p align="center">
  <img src="/images/hardware_run.png" alt="Execution of an operation on hardware.">
</p><br>Therefore, evaluating performance requires simultaneous consideration of <strong>memory bandwidth</strong> and <strong>processing unit capabilities</strong>. If a layer involes extensive computation but minimal memory access, it is termed a computation bottleneck. Conversely, if a layer requires frequent memory access but minimal computation, it is termed a memory bottleneck. We can clearly distinguish between these two scenarios according to the Roofline Model.</p>
<h1 id="Roofline-Model"><a href="#Roofline-Model" class="headerlink" title=" Roofline Model "></a><strong> Roofline Model </strong></h1><h2 id="Plot-the-Roofline-Model"><a href="#Plot-the-Roofline-Model" class="headerlink" title="Plot the Roofline Model"></a>Plot the Roofline Model</h2><p>Firstly, we need to determine the <strong>Peak Computational Performance</strong> (operations per second, OPS) $\pi$ and <strong>Peak Memory Bandwidth</strong>(bytes per second) $\beta$ specific to the target hardware device. Create a graph with <strong>performance</strong> (OPS) on the y-axis and <strong>arithmetic intensity</strong> (OPs/byte) on the x-axis:</p>
<ul>
<li>Draw a horizontal line equal to the peak computational performance, representing the maximum achievable performance by hardware. </li>
<li>Draw a diagonal line from the original point with a slope equal to the peak memory bandwidth, representing the maximum memory bandwidth avaible on the system.<table align="center">
<tr>
  <td align="center">
    <img src="/images/pi.png" alt="Peak Computational Capability" width="100%">
  </td>
  <td align="center">
    <img src="/images/beta.png" alt="Peak Memory Bandwidth" width="100%">
  </td>
</tr>
</table>

</li>
</ul>
<p>After plotting these two lines, we can get <strong>Roofline Model</strong> as below:<br><img src="/images/roofline.png" alt="Roofline Model"></p>
<h2 id="Analyze-Performance-for-Layers"><a href="#Analyze-Performance-for-Layers" class="headerlink" title="Analyze Performance for Layers"></a>Analyze Performance for Layers</h2><p>For each layer in LLMs, we can calculate its <strong>arithmetic intensity</strong>(OPs/byte) by dividing the required operations(OPs) by the amount of data transferred(bytes). According to the Roofline Model just plotted, the theoretical maximum performance for each layer is determined by the position on the graph corresponding to the arithmetic intensity of the layer. It allows us to ascertain whether this layer is <strong>compute-bound</strong> or <strong>memory-bound</strong>:</p>
<ul>
<li>If the layer’s arithmetic intensity is below the turning point, it means that the computational workload per memory access is low. Even saturating the memory bandwidth, it does not utilize the full computational capability of the hardware. In this case, the layer is constrained by memory access, and it is termed <strong>memory-bound</strong>. If the layer is memory-bound, we can optimize the performance by <strong>quantization, kernel fusion or increasing the batch size</strong>(decrease the number of memory access).</li>
<li>Conversely, if the layer’s arithmetic intensity is above the turning point, it means that the computational workload per memory access is high. It implies that the layer requires only a small amount of memory access to consume a significant amount of computational capability, and it is termed <strong>compute-bound</strong>. </li>
</ul>
<h2 id="Estimated-Execution-Time"><a href="#Estimated-Execution-Time" class="headerlink" title="Estimated Execution Time"></a>Estimated Execution Time</h2><p>The execution time is the number of operations divided by performance</p>
<div>
$$
\begin{aligned}
\text{T} &= \frac{C}{P} = \frac{C}{\min(\pi, \beta \cdot I)} \\
\end{aligned}
$$
</div>
Memory Bound:

<div>
$$
\begin{aligned}
\text{T} &= \frac{C}{\beta \cdot I} = \frac{C}{\beta \cdot \frac{C}{M}} = \frac{M}{\beta}\\
\end{aligned}
$$
</div>

<p>Compute Bound:</p>
<div>
$$
\begin{aligned}
\text{T} &= \frac{C}{\pi} \\
\end{aligned}
$$
</div>

<h2 id="Example-Usage-of-Roofline-Model"><a href="#Example-Usage-of-Roofline-Model" class="headerlink" title="Example Usage of Roofline Model"></a>Example Usage of Roofline Model</h2><p>Here, I provide an example of using the Roofline Model to analyze the performance of a specific layer in a neural network. Suppose we have some operations with #FLOPs and #Memory Access as below:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Operation</th>
<th>#FLOPs (M)</th>
<th>#Memory Access (MB)</th>
</tr>
</thead>
<tbody>
<tr>
<td>op1</td>
<td>57.8M</td>
<td>25MB</td>
</tr>
<tr>
<td>op2</td>
<td>51.4M</td>
<td>2.3MB</td>
</tr>
<tr>
<td>op3</td>
<td>925M</td>
<td>25MB</td>
</tr>
<tr>
<td>op4</td>
<td>236M</td>
<td>3.7MB</td>
</tr>
<tr>
<td>op5</td>
<td>172M</td>
<td>1.3MB</td>
</tr>
<tr>
<td>op6</td>
<td>231M</td>
<td>1.3MB</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Reduce-Memory-Access-when-it’s-Memory-Bound-op1-vs-op2"><a href="#Reduce-Memory-Access-when-it’s-Memory-Bound-op1-vs-op2" class="headerlink" title="Reduce #Memory Access when it’s Memory Bound (op1 vs op2)"></a>Reduce #Memory Access when it’s <strong>Memory Bound</strong> (op1 vs op2)</h3><ul>
<li>Reducing #FLOPs results no speed up. (refer op1 and op3)</li>
<li>Higher computational capability(higher $\pi$) does not improve performance. </li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Operation</th>
<th>#FLOPs (M)</th>
<th>#Memory Access (MB)</th>
<th>Operational Intensity</th>
<th>Attainable Performance (GFLOPS)</th>
<th>Theoretical Inference Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>op1</td>
<td>57.8M</td>
<td>24.5MB</td>
<td>2.4</td>
<td>226.5</td>
<td>255</td>
</tr>
<tr>
<td>op2</td>
<td>51.4M</td>
<td>2.3MB</td>
<td>22.5</td>
<td>2156.7</td>
<td>23.8</td>
</tr>
<tr>
<td>op3</td>
<td>925M</td>
<td>24.5MB</td>
<td>37.7</td>
<td>3622.6</td>
<td>255</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Reduce-FLOPs-when-it’s-Compute-Bound-op5-vs-op6"><a href="#Reduce-FLOPs-when-it’s-Compute-Bound-op5-vs-op6" class="headerlink" title="Reduce #FLOPs when it’s Compute Bound (op5 vs op6)"></a>Reduce #FLOPs when it’s <strong>Compute Bound</strong> (op5 vs op6)</h3><ul>
<li>Reducing #Memory Access results no speed up. (refer op4 and op6)</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Operation</th>
<th>#FLOPs (M)</th>
<th>#Memory Access (MB)</th>
<th>Operational Intensity</th>
<th>Attainable Performance (GFLOPS)</th>
<th>Theoretical Inference Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>op4</td>
<td>236M</td>
<td>3.7MB</td>
<td>64</td>
<td>4608</td>
<td>51.2</td>
</tr>
<tr>
<td>op5</td>
<td>172M</td>
<td>1.3MB</td>
<td>132</td>
<td>4608</td>
<td>37.3</td>
</tr>
<tr>
<td>op6</td>
<td>231M</td>
<td>1.3MB</td>
<td>174</td>
<td>4608</td>
<td>50.3</td>
</tr>
</tbody>
</table>
</div>
<p>From these two scenarios, we can observe that <strong>smaller memory access</strong> or <strong>smaller FLOPs</strong> will not necessarily lead to better performance. </p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h1><p>During the user’s interative with LLMs, the <strong>prefill stage</strong> excutes only once, while the <strong>decode stage</strong> is repeatedly performed to progressively generate the output. Moreover, when calculating the arithmetic intensity of each layer in LLMs, we will observe that the Prefill stage is almost entirely compute-bound, while the Decode stage is memory-bound. Therefore, optimizing for the memory bound characteristics of the decode stage becomes crucial for enhancing the inference performance of LLMs.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><strong>Reference</strong></h1><ol>
<li><a href="https://arxiv.org/pdf/2402.16363">LLM Inference Unveiled: Survey and Roofline Model Insights</a></li>
<li>Course slides of <em>Edge AI</em>, NYCU.</li>
</ol>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>AI Efficiency</tag>
      </tags>
  </entry>
  <entry>
    <title>我的第一篇貼文</title>
    <url>/2024/08/24/test/</url>
    <content><![CDATA[<p>這是一篇測試用的文章。<br><span id="more"></span></p>
<h2 id="首先先來測試一些基本的文字"><a href="#首先先來測試一些基本的文字" class="headerlink" title="首先先來測試一些基本的文字"></a>首先先來測試一些基本的文字</h2><p>嗨，你好。<br>I am Chung Pang Chun, and the handle I often use is bonginn.<br>1 + 1 = 2</p>
<h2 id="測試數學式"><a href="#測試數學式" class="headerlink" title="測試數學式"></a>測試數學式</h2><p>$x^n + y^n = z^n$</p>
<p>\begin{equation}<br>  \begin{aligned}<br>    \frac{-b + \sqrt{b^2 - 4ac}}{2a}<br>  \end{aligned}<br>\end{equation}</p>
<h2 id="測試code"><a href="#測試code" class="headerlink" title="測試code"></a>測試code</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> a, b;</span><br><span class="line">    cin &gt;&gt; a &gt;&gt; b;</span><br><span class="line">    cout &lt;&lt; a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="測試圖片"><a href="#測試圖片" class="headerlink" title="測試圖片"></a>測試圖片</h2><p><img src="/images/IMG_3033.JPG" alt="烏薩奇"></p>
<h2 id="測試連結"><a href="#測試連結" class="headerlink" title="測試連結"></a>測試連結</h2><p><a href="https://codeforces.com/">codeforces</a></p>
]]></content>
  </entry>
  <entry>
    <title>TIOJ 2070</title>
    <url>/2024/08/24/tioj2070/</url>
    <content><![CDATA[<p>Solution of TIOJ 2070 - Special Judge。<br><span id="more"></span></p>
<h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>給你由小寫英文字母組成的字串$X$，請判斷是不是每一個前$K$個英文字母的子集都是$X$的子序列。</p>
<h2 id="Observation-1"><a href="#Observation-1" class="headerlink" title="Observation 1"></a>Observation 1</h2><p>「前$k$個英文字母的『子集』的排列」都是子序列，其實就相當於要你判斷「是否前$k$個英文字母的的排列」都是子序列<br>（abc的所有排列都是，那a, b, c, ab, ac, bc, abc也當然都會是）。</p>
<h2 id="Observation-2"><a href="#Observation-2" class="headerlink" title="Observation 2"></a>Observation 2</h2><p>範圍：$K \le 20$ $\rightarrow$位元DP！</p>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>定義dp[S] = 當狀態為S時，字串所需的最短長度能夠使得滿足題目的要求，當dp[S]為n+1時，表示狀態S無法滿足。<br>另外定義pos[i][j]表示在位置j往右出現字元i最近的位置，-1表示右邊找不到i。<br>轉移式：$dp[i + (1 &lt;&lt; j)] = max(dp[i + (1 &lt;&lt; j)], pos[j][dp[i]]);$<br>轉移過程中只要有出現了任何一個pos = -1就輸出$No$，否則就輸出$Yes$。</p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> int long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pb push_back</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> all(x) (x).begin(), (x).end()</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> fastio cin.tie(0); ios_base::sync_with_stdio(false);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> INF 1e15+9</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">2e5</span><span class="number">+5</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> MOD = <span class="number">1e9</span><span class="number">+7</span>; <span class="comment">// 998244353;</span></span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>,<span class="type">int</span>&gt; P;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> k;</span><br><span class="line">    string s;</span><br><span class="line">    cin &gt;&gt; k &gt;&gt; s;</span><br><span class="line">    <span class="type">char</span> myChar[<span class="number">1005</span>] = &#123;&#125;;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=s.<span class="built_in">size</span>();i++)</span><br><span class="line">        myChar[i] = s[i<span class="number">-1</span>];</span><br><span class="line">    <span class="type">int</span> pos[<span class="number">50</span>][<span class="number">1001</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;k;i++)&#123;</span><br><span class="line">        <span class="type">int</span> curPos = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=s.<span class="built_in">size</span>();j&gt;=<span class="number">0</span>;j--)&#123;</span><br><span class="line">            pos[i][j] = curPos;</span><br><span class="line">            <span class="keyword">if</span>(myChar[j] - <span class="string">&#x27;a&#x27;</span> == i)</span><br><span class="line">                curPos = j;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> S = (<span class="number">1</span> &lt;&lt; k);</span><br><span class="line">    <span class="type">int</span> dp[S<span class="number">+5</span>] = &#123;&#125;; <span class="comment">// dp[s] = 狀態s時, 會往右延伸到哪裡, INF表沒辦法</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;S;i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;k;j++)&#123; <span class="comment">// 看看哪些沒在狀態i裡面</span></span><br><span class="line">            <span class="keyword">if</span>((i &gt;&gt; j) % <span class="number">2</span> == <span class="number">0</span>)&#123;</span><br><span class="line">                <span class="type">int</span> nearest = pos[j][dp[i]];</span><br><span class="line">                <span class="keyword">if</span>(nearest == <span class="number">-1</span>)&#123;</span><br><span class="line">                    cout &lt;&lt; <span class="string">&quot;No&quot;</span> &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;   </span><br><span class="line">                dp[i + (<span class="number">1</span> &lt;&lt; j)] = <span class="built_in">max</span>(dp[i + (<span class="number">1</span> &lt;&lt; j)], pos[j][dp[i]]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Yes&quot;</span> &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">signed</span> <span class="title">main</span><span class="params">()</span></span>&#123;fastio</span><br><span class="line">    <span class="type">int</span> T = <span class="number">1</span>;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">while</span>(T--)&#123;</span><br><span class="line">        <span class="built_in">solve</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Bonus"><a href="#Bonus" class="headerlink" title="Bonus"></a>Bonus</h2><p>這一題的前身是「構造一個最短字串滿足此性質」，此題是一個<a href="https://en.wikipedia.org/wiki/Co-NP-complete">co-NP-complete</a>的問題<br>目前能找到的最短且有系統的構造方法如下：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=k * k - <span class="number">2</span> * k + <span class="number">4</span>;i++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(i &lt;= k)</span><br><span class="line">        cout &lt;&lt; (<span class="type">char</span>)(<span class="string">&#x27;a&#x27;</span> + i - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(i &gt; k &amp;&amp; (i - <span class="number">2</span>) % (k - <span class="number">1</span>) == <span class="number">0</span>)</span><br><span class="line">        cout &lt;&lt; <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">    <span class="keyword">else</span>       </span><br><span class="line">        cout &lt;&lt; (<span class="type">char</span>)((<span class="type">int</span>)<span class="built_in">floor</span>((i - <span class="number">1</span>) * (k - <span class="number">2</span>) / (k - <span class="number">1</span>)) % (k - <span class="number">1</span>) + <span class="number">2</span> + <span class="string">&#x27;a&#x27;</span> - <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>長度：$k^2 - 2k + 4$</p>
]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
</search>
