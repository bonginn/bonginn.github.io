<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Multi-head Latent Attention</title>
    <url>/2025/12/30/mla/</url>
    <content><![CDATA[<p>This is a note about DeepSeek-V2 Multi-Head Latent Attention.<br><span id="more"></span></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Large Language Models (LLMs) operate on a simple principle: <strong>Autoregressive Decoding</strong>. This means that the model generates text one token at a time, using the entire history of the conversation to predict the next word. However, this comes with a significant computational cost. To generate N-th token, the model needs the information of all previous N-1 tokens. Re-computing the entire attention states (Key and Value) at every single step would be prohibitively slow.<br>To address this issue, we usually use <strong>KV Cache</strong> to store the Key(K) and Value(V) vectors of all previous tokens in VRAM. At each decoding step, we only need to compute the K and V vectors for the current token, and append them into the KV Cache.<br>While caching K and V avoids re-computing, it introduces a memory bottleneck that scales linearly with sequence length:</p>
<div>
$$
\begin{aligned}
\text{KV Cache Size} &= 2 \times \text{Batch Size} \times \text{Sequence Length} \times \text{Layers}\times \text{Heads} \times \text{Hidden Size} \times \text{Type Width}\\
\end{aligned}
$$
</div>

<p>In practice, modern LLM inference is often <strong>memory-bandwidth bound</strong> rather than <strong>compute-bound</strong>, especially during long-context decoding stage.<br>While attention computation itself is highly optimized, repeatedly reading and writing large KV cache from GPU memory becomes the bottleneck. This observation motivates a series of techniques to reduce the memory footprint of the KV cache, such as MQA, GQA and MLA.</p>
<h1 id="Existing-KV-Cache-Reduction-Techniques"><a href="#Existing-KV-Cache-Reduction-Techniques" class="headerlink" title="Existing KV Cache Reduction Techniques"></a>Existing KV Cache Reduction Techniques</h1><h2 id="Multi-Head-Attention-MHA"><a href="#Multi-Head-Attention-MHA" class="headerlink" title="Multi-Head Attention (MHA)"></a>Multi-Head Attention (MHA)</h2><p>In standard multi-head attention, each attention head maintains its own key and value projections.<br>For a sequence of length (L) and a model with (N) layers and (H) attention heads, the KV cache stores a distinct key-value pair for every token, layer, and head.</p>
<div>
$$
\begin{aligned}
\text{KV}_{\text{MHA}} \propto L \times N \times H \times d_h\\
\end{aligned}
$$
</div>

<p>While this design provides strong representational capacity, it leads to a large memory footprint during autoregressive decoding.<br>As the sequence length grows, reading and writing the KV cache becomes a dominant memory-bandwidth bottleneck.</p>
<h2 id="Multi-Query-Attention-MQA"><a href="#Multi-Query-Attention-MQA" class="headerlink" title="Multi-Query Attention (MQA)"></a>Multi-Query Attention (MQA)</h2><p>Multi-Query Attention reduces the KV cache size by sharing a single set of keys and values across all query heads.<br>Each attention head still computes its own query, but all heads utilize the same key-value pairs.</p>
<div>
$$
\begin{aligned}
\text{KV}_{\text{MQA}} \propto L \times N \times d_h\\
\end{aligned}
$$
</div>

<p>By removing the dependency on the number of attention heads, MQA significantly reduces the memory footprint of the KV cache.<br>However, this aggressive sharing limits the diversity of attention patterns across heads, which may degrade model quality in some settings.</p>
<h2 id="Grouped-Query-Attention-GQA"><a href="#Grouped-Query-Attention-GQA" class="headerlink" title="Grouped Query Attention (GQA)"></a>Grouped Query Attention (GQA)</h2><p>Grouped-Query Attention is similar to MQA, but it generalizes MQA by allowing multiple groups of query heads, where each group shares a set of keys and values.</p>
<div>
$$
\begin{aligned}
\text{KV}_{\text{GQA}} \propto L \times N \times H_{kv} \times d_h\\
\end{aligned}
$$
</div>

<p>where $H_{kv}$ is the number of key-value heads, and $H_q$ is the number of query heads.</p>
<p>Compared to MHA, GQA reduces the KV cache size by a constant factor of $\frac{H_{kv}}{H_q}$, while preserving more expressive power than MQA. We can see that GQA is a generalization of MQA. As a result, GQA has been widely adopted in large-scale models to balance memory efficiency and model quality.</p>
<p>Both MQA and GQA reduce the KV cache size by decreasing the number of heads stored. MLA, however, takes a completely different approach: instead of cutting down the number of heads, it changes the representation of the cache itself. The question shifts from <strong>how many heads to cache</strong> to <strong>what to cache</strong>.</p>
<h1 id="Multi-Head-Latent-Attention-MLA"><a href="#Multi-Head-Latent-Attention-MLA" class="headerlink" title="Multi-Head Latent Attention (MLA)"></a>Multi-Head Latent Attention (MLA)</h1><h2 id="Low-Rank-Key-Value-Joint-Compression"><a href="#Low-Rank-Key-Value-Joint-Compression" class="headerlink" title="Low-Rank Key-Value Joint Compression"></a>Low-Rank Key-Value Joint Compression</h2><p>The core innovation of MLA lies in its compression strategy. Unlike standard MHA where each head has a distinct projection matrix for Keys and Values, MLA projects the input hidden state into a <strong>joint compressed latent vector</strong>.<br>Let $h_t$ be the input hidden state at time $t$. MLA performs a down-projection to obtain the compressed latent vector $c_{\text{KV}}$: </p>
<div>$$\mathbf{c}_t^{KV} = W^{DKV} \mathbf{h}_t$$</div>

<p>Here, $\mathbf{c}_t^{KV} \in \mathbb{R}^{d_c}$ is the compressed latent vector, and $d_c$ is the compression dimension. Crucially, $d_c$ is significantly smaller than the total dimension of all heads in standard MHA ($d_c \ll d_h n_h$).<br>To generate the Keys and Values used for attention computation, we mathematically “up-project” this latent vector:</p>
<div>
\[
\begin{aligned}
\left[\mathbf{k}_{t,1}^C;\, \mathbf{k}_{t,2}^C;\, \dots;\, \mathbf{k}_{t,n_h}^C\right]
&= \mathbf{k}_t^C
= W^{UK} \mathbf{c}_t^{KV} \\
\left[\mathbf{v}_{t,1}^C;\, \mathbf{v}_{t,2}^C;\, \dots;\, \mathbf{v}_{t,n_h}^C\right]
&= \mathbf{v}_t^C
= W^{UV} \mathbf{c}_t^{KV}
\end{aligned}
\]
</div>

<p>where $W^{UK} \in \mathbb{R}^{d_h n_h \times d_c}$ and $W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$ are up-projection matrices.</p>
<p>Moreover, in order to reduce the activation memory during training, MLA also performs low-rank compression for the queries, even if it cannot reduce the KV cache:</p>
<div>
\[
\begin{aligned}
\mathbf{c}_t^Q
&= W^{DQ} \mathbf{h}_t \\
\left[\mathbf{q}_{t,1}^C;\, \mathbf{q}_{t,2}^C;\, \dots;\, \mathbf{q}_{t,n_h}^C\right]
&= \mathbf{q}_t^C
= W^{UQ} \mathbf{c}_t^Q
\end{aligned}
\]
</div>

<p>To demonstrate the difference between MHA, MQA, GQA and MLA, you can refer to the figure below:</p>
<p align="center">
  <img src="/images/dsattn-1.png" alt="Difference between MHA, MQA, GQA and MLA">
</p>

<h2 id="Matrix-Absorption"><a href="#Matrix-Absorption" class="headerlink" title="Matrix Absorption"></a>Matrix Absorption</h2><p>In addition, during inference, we can apply <strong>Matrix Absorption</strong> to reduce computation overhead.</p>
<p>Instead of performing the up-projections to generate the full $\mathbf{q}_{t,i}^C$ and $\mathbf{k}_{j,i}^C$ vectors, we can mathematically merge the two projection steps. Let $W^{UQ}_i$ and $W^{UK}_i$ be the up-projection sub-matrices for the $i$-th head. The attention score can be derived as the interaction between the Latent Query and Latent Key:</p>
<div>$$\begin{aligned}
\text{Score}_{t,j,i}
&amp;= (\mathbf{q}_{t,i}^C)^\top \mathbf{k}_{j,i}^C \\
&amp;= (W^{UQ}_i \mathbf{c}_t^Q)^\top (W^{UK}_i \mathbf{c}_j^{KV}) \\
&amp;= (\mathbf{c}_t^Q)^\top (W^{UQ}_i)^\top W^{UK}_i \mathbf{c}_j^{KV} \\
&amp;= (\mathbf{c}_t^Q)^\top \underbrace{\left( (W^{UQ}_i)^\top W^{UK}_i \right)}_{W^{QK,i}_{Absorbed}} \mathbf{c}_j^{KV}
\end{aligned}$$
</div>

<p>Here, $W_{QK,i}^{Absorbed} \in \mathbb{R}^{d_c \times d_c}$ is the pre-computed absorbed matrix for the $i$-th head.</p>
<p>Similarly, the Value Projection can also be optimized.<br>Here, we explicitly define the attention weight $\alpha_{t,j,i}$ as the Softmax of the scaled scores for the $i$-th head:</p>
<div>
$$\alpha_{t,j,i} = \text{Softmax}_j \left( \frac{\text{Score}_{t,j,i}}{\sqrt{d_h + d_h^R}} \right)$$
</div>

<blockquote>
<p>In DeepSeek-V2, the scaling factor is $\sqrt{d_h + d_h^R}$ to account for both content and RoPE dimensions (see section 3.3).</p>
</blockquote>
<p>In MLA, since $\mathbf{v}_{j, i}^C$ is generated from the latent vector $\mathbf{c}_j^{KV}$, we can rewrite the output calculation:</p>
<div>
$$\begin{aligned}
\mathbf{o}_{t, i} &amp;= \sum_{j=1}^t \alpha_{t,j,i} \mathbf{v}_{j,i}^C \\
&amp;= \sum_{j=1}^t \alpha_{t,j,i} (W^{UV}_i \mathbf{c}_j^{KV}) \\
&amp;= W^{UV}_i \underbrace{\left( \sum_{j=1}^t \alpha_{t,j,i} \mathbf{c}_j^{KV} \right)}_{\text{Latent Weighted Sum}}
\end{aligned}$$
</div>

<p>Finally, this head’s output is projected by the output matrix $W^O$. By associativity of matrix multiplication, we can merge $W^{UV}_i$ into $W^O_i$:</p>
<div>
$$\begin{aligned}
\mathbf{u}_{t,i} &amp;= W^O_i \mathbf{o}_{t,i} \\
&amp;= W^O_i \left( W^{UV}_i \sum_{j=1}^t \alpha_{t,j,i} \mathbf{c}_j^{KV} \right) \\
&amp;= \underbrace{(W^O_i W^{UV}_i)}_{W_{Absorbed}^{O,i}} \left( \sum_{j=1}^t \alpha_{t,j,i} \mathbf{c}_j^{KV} \right)
\end{aligned}$$
</div>

<blockquote>
<p>Here, $\mathbf{u}_{t,i}$ denotes the output-projected representation of the i-th attention head.<br>The final output $\mathbf{u}_t$ is obtained by concatenating all $\mathbf{u}_{t,i}$.</p>
</blockquote>
<p>Note that matrix absorption is applied only during inference. During training, we need to explicitly reconstruct the values for backpropagation.</p>
<h2 id="Decoupled-Rotary-Position-Embedding"><a href="#Decoupled-Rotary-Position-Embedding" class="headerlink" title="Decoupled Rotary Position Embedding"></a>Decoupled Rotary Position Embedding</h2><p>While the low-rank compression strategy successfully reduces the KV cache size, it introduces a critical conflict with <strong>Rotary Position Embedding (RoPE)</strong>.</p>
<h3 id="The-Conflict-RoPE-vs-Matrix-Absorption"><a href="#The-Conflict-RoPE-vs-Matrix-Absorption" class="headerlink" title="The Conflict: RoPE vs. Matrix Absorption"></a>The Conflict: RoPE vs. Matrix Absorption</h3><p>As mentioned in the DeepSeek-V2 paper, standard RoPE is position-sensitive and applies a rotation matrix to the Query and Key vectors. If we apply RoPE directly to the up-projected Keys $\mathbf{k}_j^C$, the rotation matrix $\mathcal{R}_j$ (which relates to the current token position) would be inserted between the query vector and the up-projection matrix $W^{UK}$:</p>
<div>
\[
\text{Score}_{t,j}
=
\mathbf{q}_t^\top\left(\mathcal{R}_j\, W^{UK}\mathbf{c}_j^{KV}\right)
\]
</div>

<p>Because <strong>matrix multiplication is not commutative</strong>, we cannot simply move the position-dependent rotation matrix $\mathcal{R}_j$ past the fixed parameters matrix $W^{UK}$ to merge it with $\mathbf{q}$. This implies that $W^{UK}$ cannot be absorbed into $W^Q$(more precise, $W^{UQ}$) during inference. If we were to persist with this approach, we would be forced to recompute the high-dimensional keys for all prefix tokens at every decoding step to apply the correct positional rotation, which would significantly hinder inference efficiency.</p>
<h3 id="The-Solution-Decoupled-RoPE-Strategy"><a href="#The-Solution-Decoupled-RoPE-Strategy" class="headerlink" title="The Solution: Decoupled RoPE Strategy"></a>The Solution: Decoupled RoPE Strategy</h3><p>To resolve this, DeepSeek-V2 employs a Decoupled RoPE strategy. The core idea is to separate the query and key vectors into two parts: a Content Part for semantic features and a RoPE Part for positional information. The generation of these vectors is defined as follows:</p>
<ol>
<li><p>Decoupled Queries (Multi-Head): For the queries, we generate a separate RoPE vector $\mathbf{q}_{t,i}^R$ for each attention head $i$. This maintains the multi-head diversity for positional attention.</p>
<div>
\[
\left[
\mathbf{q}_{t,1}^R;\,
\mathbf{q}_{t,2}^R;\,
\dots;\,
\mathbf{q}_{t,n_h}^R
\right]
=
\mathbf{q}_t^R
=
\mathrm{RoPE}\!\left(W^{QR}\mathbf{c}_t^Q\right)
\]
</div>
</li>
<li><p>Decoupled Keys (Shared-Head): For the keys, we generate a single RoPE vector $\mathbf{k}_t^R$ that is shared across all attention heads.</p>
</li>
</ol>
<div>
$$\mathbf{k}_t^R = \text{RoPE}(W^{KR} \mathbf{h}_t)$$
</div>

<blockquote>
<p>MLA minimizes the additional memory required to store positional information by sharing the RoPE key $\mathbf{k}_t^R$ across all heads (instead of having $n_h$ distinct RoPE keys).</p>
</blockquote>
<h3 id="Caching-during-inference"><a href="#Caching-during-inference" class="headerlink" title="Caching during inference"></a>Caching during inference</h3><p>In MLA, we do not cache the full per-head keys/values. Instead, for each past position $j$ we cache:</p>
<ul>
<li>the latent KV vector $\mathbf{c}_j^{KV} \in \mathbb{R}^{d_c}$, and</li>
<li>the shared RoPE key $\mathbf{k}_j^R \in \mathbb{R}^{d_h^R}$.</li>
</ul>
<h3 id="Final-Attention-Calculation"><a href="#Final-Attention-Calculation" class="headerlink" title="Final Attention Calculation"></a>Final Attention Calculation</h3><p>During the attention phase, the queries and keys are concatenations of their respective content and RoPE parts.<br>For the query at position $t$ and head $i$:</p>
<div>
$$\mathbf{q}_{t,i} = [\mathbf{q}_{t,i}^C \,;\, \mathbf{q}_{t,i}^R]$$
</div>

<p>For the key at position $j$ (where $j \le t$):</p>
<div>
$$\mathbf{k}_{j,i} = [\mathbf{k}_{j,i}^C \,;\, \mathbf{k}_{j}^R]$$
</div>

<p>The scaled attention score between query $t$ and key $j$ for head $i$ is computed as:</p>
<div>$$\begin{aligned}
\text{Score}_{t,j,i} &amp;= \frac{\mathbf{q}_{t,i}^\top \mathbf{k}_{j,i}}{\sqrt{d_h + d_h^R}} \\
&amp;= \frac{(\mathbf{q}_{t,i}^C)^\top \mathbf{k}_{j,i}^C + (\mathbf{q}_{t,i}^R)^\top \mathbf{k}_{j}^R}{\sqrt{d_h + d_h^R}}
\end{aligned}$$
</div>



<p>By expanding the dot product, we can see why MLA is efficient:</p>
<ol>
<li>Content Term $(\mathbf{q}_{t,i}^C)^\top \mathbf{k}_{j,i}^C$: Can be computed using Matrix Absorption (using latent vectors), avoiding explicit key reconstruction.</li>
<li>RoPE Term $(\mathbf{q}_{t,i}^R)^\top \mathbf{k}_{j}^R$: Computed explicitly using the small, cached RoPE keys.</li>
</ol>
<p>The normalized attention weights are obtained via softmax over all past positions $j$:</p>
<div>
$$\alpha_{t,j,i} = \text{Softmax}_j(\text{Score}_{t,j,i})$$
</div>

<p>Finally, the attention output for head $i$ is computed using the values (which are also reconstructed from latent vectors):</p>
<div>
$$\mathbf{o}_{t,i} = \sum_{j=1}^{t} \alpha_{t,j,i} \mathbf{v}_{j,i}^C$$
</div>

<p>The outputs from all heads are concatenated and projected to form the final output:</p>
<div>
$$\mathbf{u}_t = W^O [\mathbf{o}_{t,1}; \mathbf{o}_{t,2}; \dots; \mathbf{o}_{t,n_h}]$$
</div>

<p>The illustration below shows the MLA flow:</p>
<p align="center">
  <img src="/images/mla.png" alt="MLA Flow" width="500">
</p>


<h2 id="Comparison-of-Key-Value-Cache"><a href="#Comparison-of-Key-Value-Cache" class="headerlink" title="Comparison of Key-Value Cache"></a>Comparison of Key-Value Cache</h2><p>The table below compares the KV cache size per token across different attention mechanisms:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Attention Mechanism</th>
<th style="text-align:left">KV Cache per Token (# Elements)</th>
<th style="text-align:left">Capability</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>MHA</strong></td>
<td style="text-align:left">$2 \times n_h \times d_h \times l$</td>
<td style="text-align:left">Strong</td>
</tr>
<tr>
<td style="text-align:left"><strong>GQA</strong></td>
<td style="text-align:left">$2 \times n_g \times d_h \times l$</td>
<td style="text-align:left">Moderate</td>
</tr>
<tr>
<td style="text-align:left"><strong>MQA</strong></td>
<td style="text-align:left">$2 \times 1 \times d_h \times l$</td>
<td style="text-align:left">Weak</td>
</tr>
<tr>
<td style="text-align:left"><strong>MLA</strong></td>
<td style="text-align:left">$(d_c + d_h^R) \times l \approx \frac{9}{2} \times d_h \times l$</td>
<td style="text-align:left"><strong>Stronger</strong></td>
</tr>
</tbody>
</table>
</div>
<p>Here, $n_h$ denotes the number of attention heads, $d_h$ denotes the dimension per attention head, $l$ denotes the number of layers, $n_g$ denotes the number of groups in GQA, and $d_c$ and $d_h^R$ denote the KV compression dimension and the per-head dimension of the decoupled queries and key in MLA, respectively. The amount of KV cache is measured by the number of elements, regardless of the storage precision. For DeepSeek-V2, $d_c$ is set to $4d_h$ and $d_h^R$ is set to $\frac{d_h}{2}$. So, its KV cache is equal to GQA with only 2.25 groups, but its performance is stronger than MHA.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://arxiv.org/pdf/2405.04434">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</a></li>
<li><a href="https://blog.csdn.net/jinzhuojun/article/details/145392128">DeepSeek V2/V3中的MLA和Matrix Absorption</a></li>
</ul>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>AI Efficiency</tag>
        <tag>LLMs</tag>
      </tags>
  </entry>
  <entry>
    <title>Roofline Model for Performance Analysis</title>
    <url>/2025/03/09/roofline/</url>
    <content><![CDATA[<p>A note about Roofline Model.<br><span id="more"></span></p>
<h1 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a><strong>Definition</strong></h1><ol>
<li><strong>FLOPs</strong> : Number of Floating Point Operations. A Multiply-Add operation(MAC) is counted as 2 FLOPs.</li>
<li><p><strong>FLOPS</strong> : Floating Point Operations Per Second.</p>
<div>
$$
\begin{aligned}
\text{FLOPS} &= \frac{\text{FLOPs}}{\text{Second}} \\
\end{aligned}
$$
</div>
</li>
<li><p><strong>OPs</strong> : Number of Operations. (Not necessarily Floating Point Operations)</p>
</li>
<li><strong>OPS</strong> : Number of Operations Per Second.<div>
$$
\begin{aligned}
\text{OPS} &= \frac{\text{OPs}}{\text{Second}} \\
\end{aligned}
$$
</div></li>
<li><strong>Memory Bandwidth</strong> : The rate at which data can be read from or written to memory (Bytes per second).</li>
<li><strong>Arithmetic Intensity</strong> : The ratio of Total OPs performed to the Total Bytes moved.<div>
$$
\begin{aligned}
\text{I} &= \frac{C \text{(Computations, OPs)}}{M \text{(Memory Access, Bytes)}} \\
\end{aligned}
$$
</div>


</li>
</ol>
<h1 id="Process-Of-DNN-Inference-on-Hardware"><a href="#Process-Of-DNN-Inference-on-Hardware" class="headerlink" title="Process Of DNN Inference on Hardware"></a><strong>Process Of DNN Inference on Hardware</strong></h1><p>Step 1 (Memory Access): Moving Input/Weghts from memory to CPU/GPU.<br>Step 2 (Compute): Perform the operations(e.g. Linear, Convolution…).<br>Step 3 (Memory Access): Writing the computed activation back to memory.<br>The process can be visualized as below:</p>
<p><p align="center">
  <img src="/images/hardware_run.png" alt="Execution of an operation on hardware.">
</p><br>Therefore, evaluating performance requires simultaneous consideration of <strong>memory bandwidth</strong> and <strong>processing unit capabilities</strong>. If a layer involes extensive computation but minimal memory access, it is termed a computation bottleneck. Conversely, if a layer requires frequent memory access but minimal computation, it is termed a memory bottleneck. We can clearly distinguish between these two scenarios according to the Roofline Model.</p>
<h1 id="Roofline-Model"><a href="#Roofline-Model" class="headerlink" title=" Roofline Model "></a><strong> Roofline Model </strong></h1><h2 id="Plot-the-Roofline-Model"><a href="#Plot-the-Roofline-Model" class="headerlink" title="Plot the Roofline Model"></a>Plot the Roofline Model</h2><p>Firstly, we need to determine the <strong>Peak Computational Performance</strong> (operations per second, OPS) $\pi$ and <strong>Peak Memory Bandwidth</strong>(bytes per second) $\beta$ specific to the target hardware device. Create a graph with <strong>performance</strong> (OPS) on the y-axis and <strong>arithmetic intensity</strong> (OPs/byte) on the x-axis:</p>
<ul>
<li>Draw a horizontal line equal to the peak computational performance, representing the maximum achievable performance by hardware. </li>
<li>Draw a diagonal line from the original point with a slope equal to the peak memory bandwidth, representing the maximum memory bandwidth avaible on the system.<table align="center">
<tr>
  <td align="center">
    <img src="/images/pi.png" alt="Peak Computational Capability" width="100%">
  </td>
  <td align="center">
    <img src="/images/beta.png" alt="Peak Memory Bandwidth" width="100%">
  </td>
</tr>
</table>

</li>
</ul>
<p>After plotting these two lines, we can get <strong>Roofline Model</strong> as below:<br><img src="/images/roofline.png" alt="Roofline Model"></p>
<h2 id="Analyze-Performance-for-Layers"><a href="#Analyze-Performance-for-Layers" class="headerlink" title="Analyze Performance for Layers"></a>Analyze Performance for Layers</h2><p>For each layer in LLMs, we can calculate its <strong>arithmetic intensity</strong>(OPs/byte) by dividing the required operations(OPs) by the amount of data transferred(bytes). According to the Roofline Model just plotted, the theoretical maximum performance for each layer is determined by the position on the graph corresponding to the arithmetic intensity of the layer. It allows us to ascertain whether this layer is <strong>compute-bound</strong> or <strong>memory-bound</strong>:</p>
<ul>
<li>If the layer’s arithmetic intensity is below the turning point, it means that the computational workload per memory access is low. Even saturating the memory bandwidth, it does not utilize the full computational capability of the hardware. In this case, the layer is constrained by memory access, and it is termed <strong>memory-bound</strong>. If the layer is memory-bound, we can optimize the performance by <strong>quantization, kernel fusion or increasing the batch size</strong>(decrease the number of memory access).</li>
<li>Conversely, if the layer’s arithmetic intensity is above the turning point, it means that the computational workload per memory access is high. It implies that the layer requires only a small amount of memory access to consume a significant amount of computational capability, and it is termed <strong>compute-bound</strong>. </li>
</ul>
<h2 id="Estimated-Execution-Time"><a href="#Estimated-Execution-Time" class="headerlink" title="Estimated Execution Time"></a>Estimated Execution Time</h2><p>The execution time is the number of operations divided by performance</p>
<div>
$$
\begin{aligned}
\text{T} &= \frac{C}{P} = \frac{C}{\min(\pi, \beta \cdot I)} \\
\end{aligned}
$$
</div>
Memory Bound:

<div>
$$
\begin{aligned}
\text{T} &= \frac{C}{\beta \cdot I} = \frac{C}{\beta \cdot \frac{C}{M}} = \frac{M}{\beta}\\
\end{aligned}
$$
</div>

<p>Compute Bound:</p>
<div>
$$
\begin{aligned}
\text{T} &= \frac{C}{\pi} \\
\end{aligned}
$$
</div>

<h2 id="Example-Usage-of-Roofline-Model"><a href="#Example-Usage-of-Roofline-Model" class="headerlink" title="Example Usage of Roofline Model"></a>Example Usage of Roofline Model</h2><p>Here, I provide an example of using the Roofline Model to analyze the performance of a specific layer in a neural network. Suppose we have some operations with #FLOPs and #Memory Access as below:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Operation</th>
<th>#FLOPs (M)</th>
<th>#Memory Access (MB)</th>
</tr>
</thead>
<tbody>
<tr>
<td>op1</td>
<td>57.8M</td>
<td>25MB</td>
</tr>
<tr>
<td>op2</td>
<td>51.4M</td>
<td>2.3MB</td>
</tr>
<tr>
<td>op3</td>
<td>925M</td>
<td>25MB</td>
</tr>
<tr>
<td>op4</td>
<td>236M</td>
<td>3.7MB</td>
</tr>
<tr>
<td>op5</td>
<td>172M</td>
<td>1.3MB</td>
</tr>
<tr>
<td>op6</td>
<td>231M</td>
<td>1.3MB</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Reduce-Memory-Access-when-it’s-Memory-Bound-op1-vs-op2"><a href="#Reduce-Memory-Access-when-it’s-Memory-Bound-op1-vs-op2" class="headerlink" title="Reduce #Memory Access when it’s Memory Bound (op1 vs op2)"></a>Reduce #Memory Access when it’s <strong>Memory Bound</strong> (op1 vs op2)</h3><ul>
<li>Reducing #FLOPs results no speed up. (refer op1 and op3)</li>
<li>Higher computational capability(higher $\pi$) does not improve performance. </li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Operation</th>
<th>#FLOPs (M)</th>
<th>#Memory Access (MB)</th>
<th>Operational Intensity</th>
<th>Attainable Performance (GFLOPS)</th>
<th>Theoretical Inference Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>op1</td>
<td>57.8M</td>
<td>24.5MB</td>
<td>2.4</td>
<td>226.5</td>
<td>255</td>
</tr>
<tr>
<td>op2</td>
<td>51.4M</td>
<td>2.3MB</td>
<td>22.5</td>
<td>2156.7</td>
<td>23.8</td>
</tr>
<tr>
<td>op3</td>
<td>925M</td>
<td>24.5MB</td>
<td>37.7</td>
<td>3622.6</td>
<td>255</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Reduce-FLOPs-when-it’s-Compute-Bound-op5-vs-op6"><a href="#Reduce-FLOPs-when-it’s-Compute-Bound-op5-vs-op6" class="headerlink" title="Reduce #FLOPs when it’s Compute Bound (op5 vs op6)"></a>Reduce #FLOPs when it’s <strong>Compute Bound</strong> (op5 vs op6)</h3><ul>
<li>Reducing #Memory Access results no speed up. (refer op4 and op6)</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Operation</th>
<th>#FLOPs (M)</th>
<th>#Memory Access (MB)</th>
<th>Operational Intensity</th>
<th>Attainable Performance (GFLOPS)</th>
<th>Theoretical Inference Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>op4</td>
<td>236M</td>
<td>3.7MB</td>
<td>64</td>
<td>4608</td>
<td>51.2</td>
</tr>
<tr>
<td>op5</td>
<td>172M</td>
<td>1.3MB</td>
<td>132</td>
<td>4608</td>
<td>37.3</td>
</tr>
<tr>
<td>op6</td>
<td>231M</td>
<td>1.3MB</td>
<td>174</td>
<td>4608</td>
<td>50.3</td>
</tr>
</tbody>
</table>
</div>
<p>From these two scenarios, we can observe that <strong>smaller memory access</strong> or <strong>smaller FLOPs</strong> will not necessarily lead to better performance. </p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h1><p>During the user’s interative with LLMs, the <strong>prefill stage</strong> excutes only once, while the <strong>decode stage</strong> is repeatedly performed to progressively generate the output. Moreover, when calculating the arithmetic intensity of each layer in LLMs, we will observe that the Prefill stage is almost entirely compute-bound, while the Decode stage is memory-bound. Therefore, optimizing for the memory bound characteristics of the decode stage becomes crucial for enhancing the inference performance of LLMs.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><strong>Reference</strong></h1><ol>
<li><a href="https://arxiv.org/pdf/2402.16363">LLM Inference Unveiled: Survey and Roofline Model Insights</a></li>
<li>Course slides of <em>Edge AI</em>, NYCU.</li>
</ol>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>AI Efficiency</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>WQS 二分搜</title>
    <url>/2025/01/08/WQS%E4%BA%8C%E5%88%86%E6%90%9C/</url>
    <content><![CDATA[<p>一篇WQS二分搜的筆記，希望能讓自己不要這麼快忘記的個優化技巧。<br><span id="more"></span></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>WQS 二分搜又稱Aliens優化，是一個可以將二維的DP問題轉化成一個維度的優化技巧<br>複雜度可以從$O(N^2)$下降到$O(NlogC)$，但題目本身需要一些特性。<br>已知某函數$f(x)$為concave function，且我們有某種演算法可以在線性時間內求出$f(x) - px$的最大值及最大值<br>發生的位置($x_0$)。<br>接下來，我們畫出$p=0 - 5$的Cases :<br><img src="/images/Chart.png" alt="Credit : 4o"><br>可以發現，$x_0$隨著$p$上升逐漸下降！<br>因此若我們希望找到$f(x)$的最大值且$x&lt;=k$，我們可以對$p$做二分搜，找到最近的$p$使得$f(x) - px$的$x_0$$\le k$，最後答案就會是演算法的輸出$+ pk$。</p>
<h2 id="Informal-Proof"><a href="#Informal-Proof" class="headerlink" title="Informal Proof"></a>Informal Proof</h2><p>至於為什麼當p越大，極值發生位置會越來越前面，簡單的證明如下：<br>當$f(x)$極值發生時的必要條件是$\nabla f(x) = 0$<br>$\nabla (f(x) - px) = \nabla f(x) - \nabla px = \nabla f(x) - p = 0$<br>已知$f(x)$是concave，因此 $\nabla f(x)$ 遞減<br>因此當$p \uparrow$，$x$必須越來越小。<br>然而，WQS真正困難的部分其實是證明$f(x)$是concave的，很多時候看到題目都會靠直覺猜它是concave，然後直接使用此性質（就像猜某個greedy方式會是最佳解）。</p>
<h2 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h2><p>我覺得WQS二分搜有一個很好的intuition：<br>$f(x)$是某件事做了$x$次後得到的值，$p$就可以當作每做一次所需的成本，當成本($p$)越大，我們就不能做太多次操作（若我們的目標是maximize $f(x)$）。<br>因此！理所當然當$p$越大，$x_0$就會越小。</p>
<h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><p>直接來看一題很經典的WQS二分的題目<a href="https://tioj.ck.tp.edu.tw/problems/2039">AI-666 賺多少</a>：<br>題目敘述：給定$n$天股票價格，求只能交易$k$次的情況下最多能賺多少錢？（假設最多同時只能持有一張股票）<br>這題有一個很簡單的dp解：<br>$dp0[i][j]$表示在第$i$天，交易了$j$次且當下沒有股票的最佳解<br>$dp1[i][j]$表示在第$i$天，交易了$j$次且當下持有股票的最佳解<br>轉移式：<br>$dp0[i][j] = max(dp0[i-1][j], dp1[i-1][j-1] + a[i])$<br>$dp1[i][j] = max(dp1[i-1][j], dp0[i-1][j] - a[i])$<br>最後答案就是$dp0[n][k]$。<br>但很明顯，此做法的時間、空間複雜度為$O(n^2)$。<br>嘗試將問題轉換成「不限制交易次數」，我們可以發現轉移式就會變成：<br>$dp0[i] = max(dp0[i-1], dp1[i-1] + a[i])$<br>$dp1[i] = max(dp1[i-1], dp0[i-1] - a[i])$<br>其中，$dp0[i]$表示在第$i$天，當下沒有股票的最佳解，$dp1[i]$表示在第$i$天，當下持有股票的最佳解。<br>接著，我們嘗試將每次交易都增加一個$p$元的手續費，此時轉移式變成：<br>$dp0[i] = max(dp0[i-1], dp1[i-1] + a[i])$<br>$dp1[i] = max(dp1[i-1], dp0[i-1] - a[i] - p)$<br>我們可以在$O(n)$的時間解決這個「有手續費且沒有交易次數限制」的問題。<br>最後，我們可以對$p$做二分搜找到某個$p$，使得發生最大值時的交易次數$\leq k$<br>最終答案就會是此演算法的輸出$+ pk$。 </p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> int long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ll long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pb push_back</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> sz(x) (x).size()</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> all(x) (x).begin(), (x).end()</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> fastio cin.tie(0); ios_base::sync_with_stdio(false);</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">2e6</span><span class="number">+5</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> mod = <span class="number">1e9</span><span class="number">+7</span>; <span class="comment">// 998244353;</span></span><br><span class="line"><span class="type">const</span> ll inf = <span class="number">1ll</span> &lt;&lt; <span class="number">61</span>;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>,<span class="type">int</span>&gt; P;</span><br><span class="line"><span class="type">int</span> n, k;</span><br><span class="line"><span class="type">int</span> a[maxn];</span><br><span class="line">P dp0[maxn], dp1[maxn];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">()</span></span>&#123;</span><br><span class="line">    dp0[<span class="number">0</span>] = <span class="built_in">P</span>(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    dp1[<span class="number">0</span>] = <span class="built_in">P</span>(-inf, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">P <span class="title">cal</span><span class="params">(<span class="type">int</span> p)</span></span>&#123;</span><br><span class="line">    <span class="built_in">init</span>();</span><br><span class="line">    P res = &#123;<span class="number">0</span>, inf&#125;;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)&#123;</span><br><span class="line">        <span class="comment">// 沒股票</span></span><br><span class="line">        <span class="keyword">if</span>(dp0[i<span class="number">-1</span>] &gt; <span class="built_in">P</span>(dp1[i<span class="number">-1</span>].first + a[i] - p, dp1[i<span class="number">-1</span>].second - <span class="number">1</span>))</span><br><span class="line">            dp0[i] = dp0[i<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            dp0[i] = <span class="built_in">P</span>(dp1[i<span class="number">-1</span>].first + a[i] - p, dp1[i<span class="number">-1</span>].second - <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 有股票</span></span><br><span class="line">        <span class="keyword">if</span>(dp1[i<span class="number">-1</span>] &gt; <span class="built_in">P</span>(dp0[i<span class="number">-1</span>].first - a[i], dp0[i<span class="number">-1</span>].second))</span><br><span class="line">            dp1[i] = dp1[i<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            dp1[i] = <span class="built_in">P</span>(dp0[i<span class="number">-1</span>].first - a[i], dp0[i<span class="number">-1</span>].second);</span><br><span class="line">        res = <span class="built_in">max</span>(res, dp0[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span>&#123;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; k;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)</span><br><span class="line">        cin &gt;&gt; a[i];</span><br><span class="line">    <span class="type">int</span> l = <span class="number">0</span>, r = <span class="number">5e4</span>;</span><br><span class="line">    P res = <span class="built_in">cal</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">int</span> pp;</span><br><span class="line">    <span class="keyword">if</span>(-res.second &lt;= k)&#123;</span><br><span class="line">        cout &lt;&lt; res.first;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(l &lt;= r)&#123;</span><br><span class="line">        <span class="type">int</span> mid = (l + r) / <span class="number">2</span>;</span><br><span class="line">        res = <span class="built_in">cal</span>(mid);</span><br><span class="line">        <span class="keyword">if</span>(-res.second &gt; k)&#123;</span><br><span class="line">            pp = mid;</span><br><span class="line">            l = mid + <span class="number">1</span>;</span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            r = mid - <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">cal</span>(pp<span class="number">+1</span>).first + (pp<span class="number">+1</span>) * k;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">signed</span> <span class="title">main</span><span class="params">()</span></span>&#123;fastio</span><br><span class="line">    <span class="type">int</span> T = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">//cin &gt;&gt; T;</span></span><br><span class="line">    <span class="keyword">while</span>(T--)&#123;</span><br><span class="line">        <span class="built_in">solve</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在TIOJ上要稍微壓一點常數（e.g. 二分搜範圍）</p>
<p>WQA二分搜的過程其實有不少細節，請未來的我自行體會。<br>但就結論而言，一個簡單且可以避免錯誤的方法是「找最大的$p$使得最後的交易次數不滿足題目限制」<br>最後我們所需$p$就會是$p+1$(如果是離散的話)。 </p>
<h2 id="Related-Problems"><a href="#Related-Problems" class="headerlink" title="Related Problems"></a>Related Problems</h2><p><a href="https://zerojudge.tw/ShowProblem?problemid=h926"> 美食博覽會 (k 值加大版)</a><br><a href="https://cses.fi/problemset/task/2086">Subarray Squares</a><br><a href="https://www.luogu.com.cn/problem/P2619">Tree I</a><br><a href="https://codeforces.com/contest/958/problem/E2">E2. Guard Duty (medium)</a><br><a href="https://codeforces.com/contest/1279/problem/F">F. New Year and Handle Change</a><br><a href="https://codeforces.com/contest/739/problem/E">E. Gosha is hunting</a> </p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>115 交大資工所丙組推甄心得</title>
    <url>/2025/11/20/%E4%B8%99%E7%B5%84%E5%BF%83%E5%BE%97/</url>
    <content><![CDATA[<p>這篇文章沒有面試過程！<br><span id="more"></span></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>想了很久要不要寫這篇文，除了我認為我有點幸運到不值得參考、放榜結果神奇到不可思議外，更多的是我覺得有很多更厲害的人值得丙組正（逕）取的位置。但準備推甄資料的過程中受到太多人的幫助，抱持著善的循環的心，決定還是將自己的準備過程及心得記錄下來給學弟妹參考，也順便分享給未來的自己看。</p>
<h1 id="丙組是什麼？"><a href="#丙組是什麼？" class="headerlink" title="丙組是什麼？"></a>丙組是什麼？</h1><p>交大資工系碩士班分成以下幾個所：</p>
<ol>
<li>資訊科學與工程研究所（資科工所）</li>
<li>網路工程研究所（網工所）</li>
<li>多媒體工程研究所（多工所）</li>
<li>數據科學與工程研究所 </li>
<li>資訊安全研究所</li>
</ol>
<p>基本上以上每個所都是正所（雖然有些人會認為資科工才是最正的所），其中在推甄入學時，資科工所又分為甲組和丙丁戊組，甲組和網工、多工所一起算在資訊聯招中，丙丁戊則是獨立的管道，推甄時與其他組最大的不同是過了書審後<strong>需要面試</strong>。丙組是實作導向的組別，簡單說就是<strong>不太看成績，以實作能力為主</strong>，適合那些成績普通但有實作能力的高手們；丁戊組分別是系統與網路管理實務（系計中）和校務資訊系統技術與實務（校計中）。丙組在入學後與資甲並沒有太大的差異，都會是資科工所，唯一的差別是可以找的<a href="https://www.cs.nycu.edu.tw/education/master/advisor">教授名單</a>不同，且教授在丙組的名額也會比較少（基本上是一個）。 </p>
<p>可以到<a href="https://csdrive.cs.nycu.edu.tw/release/836ec023-938e-4b2b-9f45-a09d8985de8e">這裡</a>看更多關於丙組的介紹。</p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><ul>
<li>校系：交大資工系</li>
<li>成績：$53.37\%$ (GPA $3.84 / 4.30$)</li>
<li>專題：LLMs 量化演算法的研究</li>
<li>國科會大專生研究計畫</li>
<li>ICPC Regional 銅牌$\times$1、TOPC 銀牌$\times$2</li>
<li>CPE Expert Level 6題 ($1.7\%$)</li>
</ul>
<p>我沒有任何論文發表和業界實習的經驗，在校成績可以也可以用慘來形容，除了一點點的演算法競賽成績和大專生計畫之外，其實與一般人沒什麼太大的差別。</p>
<h1 id="推甄準備資料"><a href="#推甄準備資料" class="headerlink" title="推甄準備資料"></a>推甄準備資料</h1><h2 id="自傳"><a href="#自傳" class="headerlink" title="自傳"></a>自傳</h2><p>我的自傳分成四大部分：個人簡介、專題作品、學術研究、競賽與其他經驗</p>
<ol>
<li>個人簡介<br>我大概花了$\frac{1}{4}$到$\frac{1}{3}$頁介紹自己，簡單講自己有興趣的研究領域、對資工的熱愛以及個人特質。這部分對於申請資工所的自傳來說不是重點，因此不必花過多的篇幅，教授應該也不太會看。</li>
<li>專題作品<br>我認為這部分是自傳中最重要的部分，這也是你最可以展現自己專業的地方。在這部分我列了三個專題作品，每個專題又會花大約$100$到$150$字描述，其中也包含我的畢業專題。我認為在這部分要盡可能地展現出「可量化的實際數據」，寫「我提升了模型的推論速度」不如寫「我將模型的推論速度提升$2$倍，同時降低$50\%$記憶體使用量」，同時也可以說明使用到了什麼技術、專題規模等，如果有GitHub連結也可以附上。</li>
<li>學術研究<br>我在這部分放了我的國科會大專生研究計畫，簡單描述了我的計劃內容，研究動機，預期成果等，但計畫還在進行中，我就沒有花太多篇幅寫了。如果你有發表過論文，我認為在這部分可以花多一點篇幅，<strong>我相信丙組教授對於論文發表一定會加非常多分</strong>。</li>
<li>競賽與其他經驗<br>主要放了我比過的演算法競賽成績和我在<a href="https://www.facebook.com/NYCUPCCA/">PCCA</a>的經歷。</li>
</ol>
<p>至於版面配置的部分，我是使用Canva簡單製作，<strong>共兩頁A4大小</strong>。我認為只要字不要太小太大，版面不要太雜亂，讓人看得舒服就可以了。</p>
<h2 id="簡歷"><a href="#簡歷" class="headerlink" title="簡歷"></a>簡歷</h2><p>在我給我的導師看過我的自傳草稿後，他給我的回饋：</p>
<blockquote>
<p>Btw, I feel you have a very strong record. If you can have a CV summarizing your achievements as bullets, that will be much better. My two cents </p>
</blockquote>
<p>雖然交大資工所沒有叫你附上簡歷，但我認為在自傳的前一頁附上1-page CV 非常重要，據我的導師表示，他審聯招的自傳基本上只看第一頁，所以你用心寫的精美自傳教授可能只會花不到一分鐘看，這時候在第一頁就附上CV可以讓教授更快了解你的能力和特質。雖然說在丙組這種情況應該比較不會發生，但還是建議大家附上。</p>
<p>我的CV基本上就是把我的自傳的內容精簡成一頁的Bullet Points，每個Project只列出兩三點最重要的成果，盡可能地可以讓人在一分鐘內看完的同時還能夠大致了解你的專業能力。</p>
<h2 id="讀書計劃"><a href="#讀書計劃" class="headerlink" title="讀書計劃"></a>讀書計劃</h2><p>我發現網路上大多數人都會說交大的讀書計劃寫了教授也不會看，看了也不會信，但我認為在丙組這種實作導向的組別，<strong>讀書計劃更大的意義是展現出你對於自己有興趣的研究領域的掌握程度和未來規劃</strong>，好的讀書計劃可以讓教授更相信你對於該領域的了解和熱忱。</p>
<p>至於什麼是好的讀書計劃，根據我的經驗，要盡可能地寫的<strong>具體</strong>一點，避免寫一些空泛的東西，例如：「入學前加強英文能力」、「學習xx領域相關知識」我認為就過於籠統；相對地，若能具體點出要學習的工具或語言、要貢獻的開源專案或是預計專研的技術細節，會讓整份讀書計畫更有說服力，也能展現你對該領域的現況有足夠的認知。</p>
<h2 id="推薦信"><a href="#推薦信" class="headerlink" title="推薦信"></a>推薦信</h2><p>交大資工所規定需要至少兩封推薦信（上限是五封），如果你能找到五個人幫你寫推薦信當然最好，但我比較熟的教授就只有我的導師和專題教授，所以只有兩封。我的做法是在寄信詢問教授意願時會先附上我的自傳草稿，讓教授對你有基本的認識，教授答應後通常會要你提供一份推薦信的草稿，基本上就是以教授的視角來推薦自己，可以具體寫一些過去曾經在該教授指導下做過的專案、修過的課、合作過的計劃等等，格式大概會像是：</p>
<blockquote>
<p>本人在此誠摯推薦本系四年級學生XXX參加貴所的研究所推甄。在我過去所教授的YYY課程中，XXX展現出優異的學習態度與實作能力，並完成了ZZZ專案，成果不僅在效率上的表現相當突出，在與其他組員的合作上也展現了高度的溝通、協作能力，充分顯示出其卓越的研發與實作能力。<br>此外，XXX同學在本人的指導下於大三時完成專題研究「WWW」，他能獨立完成文獻整理、實驗設計與結果分析，展現了扎實的研究能力與批判思維。<br>… (其他事蹟)<br>我深信，XXX同學兼具學術潛力、研究能力與團隊合作精神。若能進入貴所繼續深造，必定能做出卓越的學術與實務貢獻。</p>
</blockquote>
<p>建議要早一點寄信給教授，可以的話就在暑假（七八月）就寄信詢問教授的意願，我是大概九月中才寄信給教授請他幫我寫推薦信，而我的專題教授是一個非常會趕Deadline的人，導致我是在推薦信上傳截止前一天凌晨四點才拿到我的第二封推薦信，非常刺激…，但還是非常感謝他們。</p>
<h1 id="推甄結果"><a href="#推甄結果" class="headerlink" title="推甄結果"></a>推甄結果</h1><p>最後我的丙組推甄結果是<strong>逕取</strong>，也就是不用面試就可以直接錄取。丙丁戊組的逕取生應該可以算是第一批上岸的，因為丙組會在交大第一梯次放榜前的約兩個禮拜先公布初試通過名單，而逕取生被會同時收到直接錄取的電話通知。逕取最大的優點就是可以提早找指導教授，因為丙組的教授名單比較少，且每個教授名額只有一個，因此在丙組的熱門教授會非常搶手，而我也很幸運的被我的專題指導教授用那寶貴的一個名額收留。</p>
<p>除了交大資工所丙組之外，我也申請了交大資訊聯招、數據所、清大資工所甲組、資應所、台大資工所、資工所人工智慧碩士班、網媒所，附上我的其他組的結果：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>學校</th>
<th>組別</th>
<th>結果</th>
</tr>
</thead>
<tbody>
<tr>
<td>交大</td>
<td>資丙</td>
<td><strong>逕取</strong></td>
</tr>
<tr>
<td>交大</td>
<td>資甲（資訊聯招）</td>
<td>備取207</td>
</tr>
<tr>
<td>交大</td>
<td>網工所（資訊聯招）</td>
<td>備取138</td>
</tr>
<tr>
<td>交大</td>
<td>多工所（資訊聯招）</td>
<td>備取157</td>
</tr>
<tr>
<td>交大</td>
<td>數據所</td>
<td>備取130</td>
</tr>
<tr>
<td>清大</td>
<td>資工所甲組</td>
<td>落榜</td>
</tr>
<tr>
<td>清大</td>
<td>資應所</td>
<td>初試不通過</td>
</tr>
<tr>
<td>台大</td>
<td>資工所</td>
<td>落榜</td>
</tr>
<tr>
<td>台大</td>
<td>人工智慧碩士班</td>
<td>落榜</td>
</tr>
<tr>
<td>台大</td>
<td>網媒所</td>
<td>落榜</td>
</tr>
</tbody>
</table>
</div>
<p>依我的成績台大全落榜很正常，但清大資工甲落榜和資應所沒過初試有點非常意外，果然所有需要看成績的組別都不適合我這種不會讀書的人…</p>
<h1 id="導師的推甄審查標準"><a href="#導師的推甄審查標準" class="headerlink" title="導師的推甄審查標準"></a>導師的推甄審查標準</h1><p>前陣子跟我的導師吃導聚的時候他有跟我分享了他審推甄資料的標準，雖然他是負責資訊聯招的，但還是記錄一下給大家參考：</p>
<ol>
<li>備審基本上只看一頁，因此務必在第一頁附上1-page CV。</li>
<li>會先用校名分組，台大交大一組、清大一組、成大一組、中字一組、其他，接著每個組會有一個GPA/$\%$數 $\rightarrow$ 基本審查分數的Mapping。</li>
<li>大部分是校系名+$\%$數決定，但如果有以下幾點會另外加分：<ul>
<li>發表論文：加大分</li>
<li>國科會大專生研究計畫：加大分</li>
<li>到中研院跟教授／院士實習：加分</li>
<li>競賽（ICPC、資安競賽、CPE）：加分</li>
<li>業界實習：加一點點分</li>
</ul>
</li>
<li>沒什麼在看推薦信，除非是教授「親自」幫你寫，而且寫得很好。</li>
<li>如果成績不好，基本上專題是唯一翻身機會。</li>
</ol>
<p>其中我覺得CP值最高的絕對是<strong>國科會大專生研究計畫</strong>，在教授的角度，申請這份計劃並通過或許就代表了你有一定的學術潛力和熱忱，而大專生研究計畫我覺得只要有心認真寫計畫書，且題目有一定的價值，通常都可以通過。如果有空可以再來寫一篇計畫書撰寫的心得。</p>
<p>當然，每個教授的推甄審查標準非常不一樣，但可以肯定的是，<strong>顧好成績絕對是最安全的選擇</strong>。雖然近年資工成績內卷的非常嚴重，但我認為很大一部分的原因也是因為碩班推甄審查標準的不確定性而盲目內卷，如果標準公開一致，且成績不再是唯一，我覺得站在學生的角度來說或許會是一件好事。</p>
<p>在這裡也順便分享我看到的兩個教授對於推甄審查的一些看法和建議，我覺得說得很好：</p>
<ul>
<li><a href="https://www.threads.com/@alex04072000/post/DOiSnVrExCq/%E6%88%91%E6%98%AF%E9%99%BD%E6%98%8E%E4%BA%A4%E5%A4%A7%E8%B3%87%E5%B7%A5%E7%B3%BB%E5%8A%A9%E7%90%86%E6%95%99%E6%8E%88%E4%BE%86%E8%81%8A%E8%81%8A%E6%8E%A8%E7%94%84%E8%B3%87%E6%96%99%E6%80%8E%E9%BA%BC%E6%BA%96%E5%82%99%E6%AF%8F%E5%B9%B4%E6%8E%A8%E7%94%84%E5%AD%A3%E6%88%91%E9%83%BD%E6%9C%83%E6%94%B6%E5%88%B0%E4%B8%8A%E7%99%BE%E4%BB%BD%E7%94%B3%E8%AB%8B%E5%8C%85%E5%90%AB%E7%B3%BB%E4%B8%8A%E5%AF%A9%E6%9F%A5%E8%88%87%E5%AD%B8%E7%94%9F%E9%A0%90%E7%B4%84%E9%9D%A2%E8%AB%87%E7%9C%8B%E4%BA%86%E9%80%99%E9%BA%BC%E5%A4%9A%E8%B3%87%E6%96%99%E5%BE%8C%E6%83%B3%E8%B7%9F%E5%90%8C%E5%AD%B8%E5%80%91%E5%88%86%E4%BA%AB%E5%BE%9E%E6%88%91%E7%9A%84%E8%A6%96%E8%A7%92%E4%BE%86%E7%9C%8B%E4%BB%80%E9%BA%BC%E6%89%8D%E6%98%AF%E7%9C%9F%E6%AD%A3?hl=zh-tw">劉育綸教授</a></li>
<li><a href="https://www.facebook.com/share/p/17NwUtACmK/">吳凱強教授</a></li>
</ul>
<h1 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h1><p>我自認我推甄的整個過程真的非常非常幸運，我從最一開始就決定絕對不要考試，因為我覺得考碩班是一件非常浪費生命的事情，因此全心全力沒有退路地投入在推甄的準備。交大有專門開了實作導向的組別、我的專題教授剛好也在丙組教授名單中、直接逕取不用承受面試的壓力與不確定性、教授願意收留我，這些事情如果有任何一個沒有發生，我不知道我現在會在做什麼。<br>總之還是很感謝當初幫助我的所有人，也祝大家順利！</p>
]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Misc</tag>
      </tags>
  </entry>
  <entry>
    <title>Backpropagation 反向傳播</title>
    <url>/2025/02/17/backpropagation/</url>
    <content><![CDATA[<p>學PyTorch不如自己刻一次Backpropagation。<br><span id="more"></span></p>
<h1 id="Opening"><a href="#Opening" class="headerlink" title="Opening"></a><strong>Opening</strong></h1><p>本篇文章是建立在對Gradient Descent、Neural Network架構有基礎了解的前提下，介紹如何透過Backpropagation來更新神經網路的參數，並且使用NumPy手刻出Backpropagation的過程。</p>
<h1 id="Forward-向前傳播"><a href="#Forward-向前傳播" class="headerlink" title="Forward 向前傳播"></a><strong>Forward 向前傳播</strong></h1><p>首先，先來看神經網路的基本架構：</p>
<p><p align="center">
  <img src="/images/nn.png" alt="Credit: 4o">
</p><br>這是一個簡易的神經網路，輸入、隱藏層皆有兩個Neurons加上一個Bias，輸出則有一個Neurons。對於某Neuron $X_i$，向前傳播過程可以寫成：</p>
<div>
$$
\begin{aligned}
Z^{(1)}_j &= \sum_{i=1}^{D} W^{(1)}_{ij}X_i + B^{(1)}_j \\
A^{(1)}_j &= h(Z^{(1)}_j) \\
Z^{(2)}_k &= \sum_{j=1}^{M} W^{(2)}_{jk}A^{(1)}_j + B^{(2)}_k \\
A^{(2)}_k &= Z^{(2)}_k \\

\text{E}(W,B) &= \frac{1}{2 } \sum_{k=1}^{K} (A^{(2)}_k - Y_k)^2 
\end{aligned}
$$
</div>
這裡的$h$表示隱藏層的激活函數（Activation Function），這裡因為以Regression為例，所以輸出層的激活函數為Identity Function。$\text{E}$表示Loss Function，這裡以Mean Squared Error為例。

若僅考慮Variables/Activations的Dependency，可以寫成：
<div>
$$
\begin{aligned}
X_i \rightarrow W^{(1)}_{ij} \rightarrow Z^{(1)}_j \rightarrow A^{(1)}_j \rightarrow W^{(2)}_{jk} \rightarrow Z^{(2)}_k \rightarrow A^{(2)}_k \leftrightarrow Y_k \rightarrow \text{E}
\end{aligned}
$$
</div>

<h1 id="Backward-反向傳播"><a href="#Backward-反向傳播" class="headerlink" title="Backward 反向傳播"></a><strong>Backward 反向傳播</strong></h1><p>接著，我們要來更新$W^{(2)}$的參數，我們的目標是計算$\frac{\partial E}{\partial W^{(2)}_{jk}}$，但這個東西很難直接算出來，因此我們考慮使用<strong>Chain Rule</strong>，透過上式的Dependency關係將其拆成 :</p>
<div>
$$
\begin{aligned}
\frac{\partial E}{\partial W^{(2)}_{jk}} &= \frac{\partial E}{\partial Z^{(2)}_k} \cdot \frac{\partial Z^{(2)}_k}{\partial W^{(2)}_{jk}} \\
\end{aligned}
$$  
</div>

<p>透過一些簡單的偏微分計算，我們得到：</p>
<div>
$$
\begin{aligned}
\frac{\partial E}{\partial Z^{(2)}_k} &= A^{(2)}_k - Y_k = \delta_{k} \\
\frac{\partial Z^{(2)}_k}{\partial W^{(2)}_{jk}} &= A^{(1)}_j
\end{aligned}
$$
</div>
接著再次使用Chain Rule，分解$\frac{\partial E}{\partial W^{(1)}_{ij}}$：
<div>
$$
\begin{aligned}
\frac{\partial E}{\partial W^{(1)}_{ij}} &= 
\sum_{k=1}^{K} \left(
\frac{\partial E}{\partial Z^{(2)}_k} \cdot
\frac{\partial Z^{(2)}_k}{\partial Z^{(1)}_j} \cdot
\frac{\partial Z^{(1)}_j}{\partial W^{(1)}_{ij}} \right) \\
\end{aligned}
$$
</div>
再複習一次大一微積分：
<div>
$$
\begin{aligned}
\frac{\partial E}{\partial Z^{(2)}_k} &= \delta_{k} \\
\frac{\partial Z^{(2)}_k}{\partial Z^{(1)}_j} &= W^{(2)}_{jk} \cdot h^{'}(Z^{(1)}_j) \\
\frac{\partial Z^{(1)}_j}{\partial W^{(1)}_{ij}} &= X_i
\end{aligned}
$$
</div>

<p>其中，我們定義第一層的誤差信號 $\delta_j$ 為：</p>
<div>
$$
\begin{aligned}
\delta_j &= \frac{\partial E}{\partial Z^{(1)}_j} = \sum_{k=1}^{K} W^{(2)}_{jk} \delta_k h'(Z^{(1)}_j)
\end{aligned}
$$
</div>

<p>將上述幾式整理，我們可以得到：</p>
<div>
$$
\begin{aligned}
\frac{\partial E}{\partial W^{(2)}_{jk}} &= \delta_{k} \cdot A^{(1)}_j \\
\frac{\partial E}{\partial W^{(1)}_{ij}} &= \delta_{j} \cdot X_i
\end{aligned}
$$
</div>

<p>對於Bias的Gradient，其實就是：</p>
<div>
$$
\begin{aligned}
\frac{\partial E}{\partial B^{(2)}_{k}} &= \delta_{k} \\
\frac{\partial E}{\partial B^{(1)}_{j}} &= \delta_{j}
\end{aligned}
$$
</div>

<p>如此一來我們就算完最複雜的部分了，可以進入實作了！</p>
<h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a><strong>Implementation</strong></h1><p>在開始實作前，我們先釐清各個矩陣的維度：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>符號</th>
<th>說明</th>
<th>維度</th>
</tr>
</thead>
<tbody>
<tr>
<td>$X$</td>
<td>輸入矩陣</td>
<td>(m, n_input)</td>
</tr>
<tr>
<td>$W^{(1)}$</td>
<td>第一層的權重矩陣</td>
<td>(n_input, n_hidden)</td>
</tr>
<tr>
<td>$B^{(1)}$</td>
<td>第一層的偏差矩陣</td>
<td>(1, n_hidden)</td>
</tr>
<tr>
<td>$Z^{(1)}$</td>
<td>第一層的線性組合</td>
<td>(m, n_hidden)</td>
</tr>
<tr>
<td>$A^{(1)}$</td>
<td>第一層的激活函數結果</td>
<td>(m, n_hidden)</td>
</tr>
<tr>
<td>$W^{(2)}$</td>
<td>第二層的權重矩陣</td>
<td>(n_hidden, n_output)</td>
</tr>
<tr>
<td>$B^{(2)}$</td>
<td>第二層的偏差矩陣</td>
<td>(1, n_output)</td>
</tr>
<tr>
<td>$Z^{(2)}$</td>
<td>第二層的線性組合</td>
<td>(m, n_output)</td>
</tr>
<tr>
<td>$A^{(2)}$</td>
<td>第二層的激活函數結果</td>
<td>(m, n_output)</td>
</tr>
</tbody>
</table>
</div>
<p>其中，m為Batch Size，n_input為輸入的特徵數，n_hidden為隱藏層的Neuron數，n_output為輸出的Neuron數。</p>
<h2 id="Forward"><a href="#Forward" class="headerlink" title="Forward"></a><strong>Forward</strong></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        x (numpy.ndarray): Input matrix of shape (batch_size, n_input).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        numpy.ndarray: Output matrix of shape (batch_size, n_output).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="variable language_">self</span>.z1 = np.matmul(x, <span class="variable language_">self</span>.w1) + <span class="variable language_">self</span>.b1</span><br><span class="line">    <span class="variable language_">self</span>.a1 = <span class="variable language_">self</span>.ReLU(<span class="variable language_">self</span>.z1)</span><br><span class="line">    <span class="variable language_">self</span>.z2 = np.matmul(<span class="variable language_">self</span>.a1, <span class="variable language_">self</span>.w2) + <span class="variable language_">self</span>.b2</span><br><span class="line">    <span class="variable language_">self</span>.a2 = <span class="variable language_">self</span>.z2</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.a2</span><br></pre></td></tr></table></figure>
<p>在這裡選用ReLU作為隱藏層的Activation Function。</p>
<h2 id="Backward"><a href="#Backward" class="headerlink" title="Backward"></a><strong>Backward</strong></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, X, y, pred, lr</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Parameters:</span></span><br><span class="line"><span class="string">        X (numpy.ndarray): Input matrix of shape (batch_size, n_input).</span></span><br><span class="line"><span class="string">        y (numpy.ndarray): Target matrix of shape (batch_size, n_output).</span></span><br><span class="line"><span class="string">        pred (numpy.ndarray): Output matrix of shape (batch_size, n_output).</span></span><br><span class="line"><span class="string">        lr (float): Learning rate.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># STEP 1</span></span><br><span class="line">    batch_size = X.shape[<span class="number">0</span>]</span><br><span class="line">    delta_output = <span class="number">2</span> * (pred - y) / batch_size</span><br><span class="line"></span><br><span class="line">    <span class="comment"># STEP 2</span></span><br><span class="line">    dw2 = np.matmul(<span class="variable language_">self</span>.a1.T, delta_output)</span><br><span class="line">    db2 = np.<span class="built_in">sum</span>(delta_output, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># STEP 3</span></span><br><span class="line">    delta_hidden = np.matmul(delta_output, <span class="variable language_">self</span>.w2.T) * <span class="variable language_">self</span>.ReLU_derivative(<span class="variable language_">self</span>.z1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># STEP 4</span></span><br><span class="line">    dw1 = np.matmul(X.T, delta_hidden)</span><br><span class="line">    db1 = np.<span class="built_in">sum</span>(delta_hidden, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># UPDATE</span></span><br><span class="line">    <span class="variable language_">self</span>.w1 -= lr * dw1</span><br><span class="line">    <span class="variable language_">self</span>.b1 -= lr * db1</span><br><span class="line">    <span class="variable language_">self</span>.w2 -= lr * dw2</span><br><span class="line">    <span class="variable language_">self</span>.b2 -= lr * db2</span><br></pre></td></tr></table></figure>
<p>這裡的ReLU_derivative是ReLU的導函數（也可以換成其他Activation Function，但記得要differentiable）。<br>$ \delta_k  \text{加了} \frac{2}{\text{batch_size}}$ 常數是因為真正MSE函式應為 <code>np.mean((A - Y) ** 2)</code>，這個mean是指將$\textbf{所有資料}$的Square Error取平均，而上面的MSE公式則是為了讓$\textbf{單一資料}$Gradient計算時會比較漂亮，所以乘了一個$\frac{1}{2}$。<br>當初在寫Backward時一直不懂矩陣到底要怎麼乘，但後來發現只要想清楚上面的公式，再對照好矩陣維度就可以了！</p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a><strong>Training</strong></h2><p>再來來寫一個簡單的Training Function：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y, epochs=<span class="number">1000</span>, lr=<span class="number">0.01</span>, verbose=<span class="number">10</span></span>):</span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   Parameters:</span></span><br><span class="line"><span class="string">       X (numpy.ndarray): Input matrix of shape (batch_size, n_input).</span></span><br><span class="line"><span class="string">       y (numpy.ndarray): Target matrix of shape (batch_size, n_output).</span></span><br><span class="line"><span class="string">       epochs (int): Number of epochs.</span></span><br><span class="line"><span class="string">       lr (float): Learning rate.</span></span><br><span class="line"><span class="string">       verbose (int): Frequency of printing the loss</span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br><span class="line">   <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">       pred = <span class="variable language_">self</span>.forward(X)</span><br><span class="line">       loss = <span class="variable language_">self</span>.MSE(y, pred)</span><br><span class="line">       <span class="variable language_">self</span>.backward(X, y, pred, lr)</span><br><span class="line">       <span class="keyword">if</span> epoch % verbose == <span class="number">0</span>:</span><br><span class="line">           <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.6</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h1><p>這裡我以Regression為例，但實際上只要將Loss Function、Activation Function換掉，就可以應用在其他任務上。以下是一些常見的任務：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>Tasks</strong></th>
<th><strong>輸出層 Activation Function</strong></th>
<th><strong>Loss Function</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>回歸（Regression）</strong></td>
<td><code>Identity Function</code></td>
<td><strong>Mean Squared Error</strong></td>
</tr>
<tr>
<td><strong>二元分類（Binary Classification）</strong></td>
<td><code>Sigmoid</code></td>
<td><strong>Binary Cross-Entropy</strong></td>
</tr>
<tr>
<td><strong>多類分類（Multi-Class Classification）</strong></td>
<td><code>Softmax</code></td>
<td><strong>Categorical Cross-Entropy</strong></td>
</tr>
</tbody>
</table>
</div>
<p>再做一些微分時小小的修改就可以了。但我認為重點在於理解Backpropagation中使用Chain Rule的技巧，還有計算Partial Derivative的一些細節。</p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>Revisiting Throughput</title>
    <url>/2025/03/20/throughput/</url>
    <content><![CDATA[<p>This is a note about throughput and latency from the perspective of <strong>Software</strong> and <strong>Hardware</strong>.<br><span id="more"></span></p>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a><strong>Overview</strong></h1><p>In the scenario of analyzing the performance of a DNN model, <strong>throughput</strong> and <strong>latency</strong> are two fundamental metrics. While these terms are often understood at a high level, when considering them from both hardware and software perspectives at the same time, there are important factors that are often overlooked.</p>
<h1 id="Basic-Concepts-about-Throughput-and-Latency"><a href="#Basic-Concepts-about-Throughput-and-Latency" class="headerlink" title="Basic Concepts about Throughput and Latency"></a><strong>Basic Concepts about Throughput and Latency</strong></h1><p><strong> Throughput </strong> is defined as the amount of data that can be processed/completed in a given time period. We can report it as : </p>
<div>
$$
\begin{align}
\text{Throughput} &= \frac{\text{Number of operations}}{\text{seconds}} \tag{1} \\
\end{align} 
$$
</div>

<p>Applications require high throughput when a system involves large-scale data processing, where maximizing the number of processed units per second is critical. For example, real-time video processing, security and surveillance, medical diagnosis, and drug discovery all benefit from high throughput.</p>
<p><strong> Latency </strong> is the time taken to complete a single operation. We can report it as :</p>
<div>
$$
\begin{align}
\text{Latency} &= \text{Time}_{\text{output generated}} - \text{Time}_{\text{input arrives to a system}} \tag{2}\\
\end{align}
$$
</div>

<p>In general, real-time interative applications, such as AR, autonomous navigation and robotics, require low latency to ensure a smooth user experience.</p>
<h1 id="A-Deeper-Look-at-Throughput"><a href="#A-Deeper-Look-at-Throughput" class="headerlink" title="A Deeper Look at Throughput"></a><strong>A Deeper Look at Throughput</strong></h1><h2 id="Factors-Affecting-Throughput"><a href="#Factors-Affecting-Throughput" class="headerlink" title="Factors Affecting Throughput"></a><strong>Factors Affecting Throughput</strong></h2><p>In model inference perspective, throughput is considered as inference per second. Firstly, we can break it down into two components:</p>
<div>
$$
\begin{align}
\frac{\text{inferences}}{\text{seconds}} &= \frac{\text{operations}}{\text{seconds}} \times \frac{1}{\frac{\text{operations}}{\text{inferences}}} \tag{3} \\
\end{align}
$$
</div>

<p>The denominator of second term is dictated by <strong> only DNN models </strong>, because it is defined as how many operations are required to complete a single inference.<br>For the first term, we can further break it down:</p>
<div>
$$
\begin{align}
\frac{\text{operations}}{\text{seconds}} = 
\left( \frac{1}{\frac{\text{cycles}}{\text{operations}}} \times \frac{\text{cycles}}{\text{second}} \right)
\times \text{number of PEs} \times \text{utilization of PEs} \tag{4}
\end{align}
$$
</div>

<p>Here, we consider a system comprised of multiple processing elements (PEs), where PE means a simple or  primitive core that performs a single <strong>MAC</strong> operation.</p>
<p align="center">
  <img src="/images/pe.png" alt="Single PE setup" width="50%">
</p>

<p>Let’s take a closer look at each term:</p>
<ol>
<li>$\frac{1}{\frac{\text{cycles}}{operation}}$ is <strong> IPC </strong> (Instructions per Cycle), which is the number of operations that can be completed in a single cycle.</li>
<li>$\frac{\text{cycles}}{\text{second}}$ is the <strong> clock rate </strong> of the system.<br>The combination of these two terms is the peak throughput of the system, which is the maximum number of operations that can be completed in a second.</li>
<li>$\text{number of PEs}$ means the amount of parallelism in the system.</li>
<li>$\text{utilization of PEs}$ is the fraction of time that the PEs are busy performing operations.<br>This term is determined by DNN model(i.e. memory-bound or not).</li>
</ol>
<p>Back to equation (3), we can say that the first term is dictated by DNN hardware and DNN models, while the second term is dictated by the DNN model.</p>
<h2 id="Increase-Throughput"><a href="#Increase-Throughput" class="headerlink" title="Increase Throughput"></a><strong>Increase Throughput</strong></h2><p>If we want to increase throughput from a hardware perspective, we can consider the following:</p>
<ol>
<li><p>Increase $\frac{\text{cycle}}{\text{second}}$ $\rightarrow$ higher clock rate $\rightarrow$ reducing critical path.</p>
<ul>
<li>Affected by design of the MAC.</li>
</ul>
</li>
<li><p>Increase number of PEs $\rightarrow$ multiple MACs perform in parallel.</p>
<ul>
<li>If area cost of the system is fixed<ul>
<li>Increasing the area density of the PEs.</li>
<li>Trading off <strong>on-chip storage</strong> area for more PEs.</li>
</ul>
</li>
<li>Increasing the density of PEs<ul>
<li>Reducing the logic associated with delivering operands to a MAC.<ul>
<li>Multiple MACs share control logic.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>$\text{utilization of PEs} = \frac{\text{number of active PEs}}{\text{number of PEs}} \times \text{utilization of active PEs}$<br>First term means <strong>ability to distribute the workload to PEs</strong>. (在做事的PE/所有PE)<br>Second term means <strong>how efficiently those active PEs are processing the workload.</strong> (做有用的事情的PE)</p>
</li>
</ol>
<h2 id="Effectual-and-Ineffectual-Opertaions"><a href="#Effectual-and-Ineffectual-Opertaions" class="headerlink" title="Effectual and Ineffectual Opertaions"></a><strong>Effectual and Ineffectual Opertaions</strong></h2><p>If we consider effectual and ineffectual operations, we can further break down the first term of equation (4) as follows:</p>
<div>
$$
\begin{align}
\frac{\text{operations}}{\text{cycle}} = \frac{\text{EO} + \text{UIO}}{\text{cycle}} \times \frac{\text{EO}}{\text{EO} + \text{UIO}} \times \frac{1}{\frac{\text{EO}}{\text{operations}}} \tag{5}
\end{align}
$$
</div>

<p>where $\textbf{EO}$ is <strong>effectual operations</strong> and $\textbf{UIO}$ is <strong>unexploited ineffectual operations</strong>(e.g. MAC accumulates anything multiplied by zero $\rightarrow$ $\color{red}{\text{sparsity}}$).<br>In equation (5), the first term is a constant for a given hardware accelerator design, the second term is the ability of the hardware to exploit the ineffectual operations(硬體發覺無效計算的能力), and the last term is related to amount of sparsity and depends on the DNN model.</p>
<h2 id="Reducing-Precision-e-g-Quantization"><a href="#Reducing-Precision-e-g-Quantization" class="headerlink" title="Reducing Precision (e.g. Quantization)"></a><strong>Reducing Precision</strong> (e.g. Quantization)</h2><p>Lowering precision of computation, such as through quantization is a common optimization technique. Lower precision means fewer bits to process per operation, which reduces the bandwidth requirements. This allows for higher utilization of PEs, ultimately leading to increased throughput(i.e. more operations per second).<br>However, supporting multiple levels of precision require additional hardware, which may increase the critical path and reduces the area density of PEs. This results in a decrease in overall throughput.</p>
<h2 id="Factors-that-Affect-Inference-per-Second"><a href="#Factors-that-Affect-Inference-per-Second" class="headerlink" title="Factors that Affect Inference per Second"></a><strong>Factors that Affect Inference per Second</strong></h2><p>Based on the above discussion, we can summarize the factors that affect inference per second as this table:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Factor</th>
<th>Hardware</th>
<th>DNN Models</th>
<th>Input Data</th>
</tr>
</thead>
<tbody>
<tr>
<td>Operations per inference</td>
<td></td>
<td>✓</td>
<td></td>
</tr>
<tr>
<td>Operations per cycle</td>
<td>✓</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Cycles per second</td>
<td>✓</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Number of PEs</td>
<td>✓</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Number of active PEs</td>
<td>✓</td>
<td>✓</td>
<td></td>
</tr>
<tr>
<td>Utilization of active PEs</td>
<td>✓</td>
<td>✓</td>
<td></td>
</tr>
<tr>
<td>Effectual operations out of (total) operations</td>
<td></td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>Effectual operations plus unexploited ineffectual operations per cycle</td>
<td>✓</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><strong>Reference</strong></h1><ol>
<li>Course slides of <em>Edge AI</em>, NYCU.</li>
</ol>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>AI Efficiency</tag>
      </tags>
  </entry>
  <entry>
    <title>TIOJ 2070</title>
    <url>/2024/08/24/tioj2070/</url>
    <content><![CDATA[<p>Solution of TIOJ 2070 - Special Judge。<br><span id="more"></span></p>
<h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>給你由小寫英文字母組成的字串$X$，請判斷是不是每一個前$K$個英文字母的子集都是$X$的子序列。</p>
<h2 id="Observation-1"><a href="#Observation-1" class="headerlink" title="Observation 1"></a>Observation 1</h2><p>「前$k$個英文字母的『子集』的排列」都是子序列，其實就相當於要你判斷「是否前$k$個英文字母的的排列」都是子序列<br>（abc的所有排列都是，那a, b, c, ab, ac, bc, abc也當然都會是）。</p>
<h2 id="Observation-2"><a href="#Observation-2" class="headerlink" title="Observation 2"></a>Observation 2</h2><p>範圍：$K \le 20$ $\rightarrow$位元DP！</p>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>定義dp[S] = 當狀態為S時，字串所需的最短長度能夠使得滿足題目的要求，當dp[S]為n+1時，表示狀態S無法滿足。<br>另外定義pos[i][j]表示在位置j往右出現字元i最近的位置，-1表示右邊找不到i。<br>轉移式：$dp[i + (1 &lt;&lt; j)] = max(dp[i + (1 &lt;&lt; j)], pos[j][dp[i]]);$<br>轉移過程中只要有出現了任何一個pos = -1就輸出$No$，否則就輸出$Yes$。</p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> int long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> pb push_back</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> all(x) (x).begin(), (x).end()</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> fastio cin.tie(0); ios_base::sync_with_stdio(false);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> INF 1e15+9</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxn = <span class="number">2e5</span><span class="number">+5</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> MOD = <span class="number">1e9</span><span class="number">+7</span>; <span class="comment">// 998244353;</span></span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>,<span class="type">int</span>&gt; P;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> k;</span><br><span class="line">    string s;</span><br><span class="line">    cin &gt;&gt; k &gt;&gt; s;</span><br><span class="line">    <span class="type">char</span> myChar[<span class="number">1005</span>] = &#123;&#125;;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=s.<span class="built_in">size</span>();i++)</span><br><span class="line">        myChar[i] = s[i<span class="number">-1</span>];</span><br><span class="line">    <span class="type">int</span> pos[<span class="number">50</span>][<span class="number">1001</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;k;i++)&#123;</span><br><span class="line">        <span class="type">int</span> curPos = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=s.<span class="built_in">size</span>();j&gt;=<span class="number">0</span>;j--)&#123;</span><br><span class="line">            pos[i][j] = curPos;</span><br><span class="line">            <span class="keyword">if</span>(myChar[j] - <span class="string">&#x27;a&#x27;</span> == i)</span><br><span class="line">                curPos = j;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> S = (<span class="number">1</span> &lt;&lt; k);</span><br><span class="line">    <span class="type">int</span> dp[S<span class="number">+5</span>] = &#123;&#125;; <span class="comment">// dp[s] = 狀態s時, 會往右延伸到哪裡, INF表沒辦法</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;S;i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;k;j++)&#123; <span class="comment">// 看看哪些沒在狀態i裡面</span></span><br><span class="line">            <span class="keyword">if</span>((i &gt;&gt; j) % <span class="number">2</span> == <span class="number">0</span>)&#123;</span><br><span class="line">                <span class="type">int</span> nearest = pos[j][dp[i]];</span><br><span class="line">                <span class="keyword">if</span>(nearest == <span class="number">-1</span>)&#123;</span><br><span class="line">                    cout &lt;&lt; <span class="string">&quot;No&quot;</span> &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;   </span><br><span class="line">                dp[i + (<span class="number">1</span> &lt;&lt; j)] = <span class="built_in">max</span>(dp[i + (<span class="number">1</span> &lt;&lt; j)], pos[j][dp[i]]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Yes&quot;</span> &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">signed</span> <span class="title">main</span><span class="params">()</span></span>&#123;fastio</span><br><span class="line">    <span class="type">int</span> T = <span class="number">1</span>;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">while</span>(T--)&#123;</span><br><span class="line">        <span class="built_in">solve</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Bonus"><a href="#Bonus" class="headerlink" title="Bonus"></a>Bonus</h2><p>這一題的前身是「構造一個最短字串滿足此性質」，此題是一個<a href="https://en.wikipedia.org/wiki/Co-NP-complete">co-NP-complete</a>的問題<br>目前能找到的最短且有系統的構造方法如下：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=k * k - <span class="number">2</span> * k + <span class="number">4</span>;i++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(i &lt;= k)</span><br><span class="line">        cout &lt;&lt; (<span class="type">char</span>)(<span class="string">&#x27;a&#x27;</span> + i - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(i &gt; k &amp;&amp; (i - <span class="number">2</span>) % (k - <span class="number">1</span>) == <span class="number">0</span>)</span><br><span class="line">        cout &lt;&lt; <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">    <span class="keyword">else</span>       </span><br><span class="line">        cout &lt;&lt; (<span class="type">char</span>)((<span class="type">int</span>)<span class="built_in">floor</span>((i - <span class="number">1</span>) * (k - <span class="number">2</span>) / (k - <span class="number">1</span>)) % (k - <span class="number">1</span>) + <span class="number">2</span> + <span class="string">&#x27;a&#x27;</span> - <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>長度：$k^2 - 2k + 4$</p>
]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title>我的第一篇貼文</title>
    <url>/2024/08/24/test/</url>
    <content><![CDATA[<p>這是一篇測試用的文章。<br><span id="more"></span></p>
<h2 id="首先先來測試一些基本的文字"><a href="#首先先來測試一些基本的文字" class="headerlink" title="首先先來測試一些基本的文字"></a>首先先來測試一些基本的文字</h2><p>嗨，你好。<br>I am Chung Pang Chun, and the handle I often use is bonginn.<br>1 + 1 = 2</p>
<h2 id="測試數學式"><a href="#測試數學式" class="headerlink" title="測試數學式"></a>測試數學式</h2><p>$x^n + y^n = z^n$</p>
<p>\begin{equation}<br>  \begin{aligned}<br>    \frac{-b + \sqrt{b^2 - 4ac}}{2a}<br>  \end{aligned}<br>\end{equation}</p>
<h2 id="測試code"><a href="#測試code" class="headerlink" title="測試code"></a>測試code</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> a, b;</span><br><span class="line">    cin &gt;&gt; a &gt;&gt; b;</span><br><span class="line">    cout &lt;&lt; a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="測試圖片"><a href="#測試圖片" class="headerlink" title="測試圖片"></a>測試圖片</h2><p><img src="/images/IMG_3033.JPG" alt="烏薩奇"></p>
<h2 id="測試連結"><a href="#測試連結" class="headerlink" title="測試連結"></a>測試連結</h2><p><a href="https://codeforces.com/">codeforces</a></p>
]]></content>
  </entry>
</search>
