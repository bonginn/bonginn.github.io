{"posts":[{"title":"WQS 二分搜","text":"一篇WQS二分搜的筆記，希望能讓自己不要這麼快忘記的個優化技巧。 IntroductionWQS 二分搜又稱Aliens優化，是一個可以將二維的DP問題轉化成一個維度的優化技巧複雜度可以從$O(N^2)$下降到$O(NlogC)$，但題目本身需要一些特性。已知某函數$f(x)$為concave function，且我們有某種演算法可以在線性時間內求出$f(x) - px$的最大值及最大值發生的位置($x_0$)。接下來，我們畫出$p=0 - 5$的Cases :可以發現，$x_0$隨著$p$上升逐漸下降！因此若我們希望找到$f(x)$的最大值且$x&lt;=k$，我們可以對$p$做二分搜，找到最近的$p$使得$f(x) - px$的$x_0$$\\le k$，最後答案就會是演算法的輸出$+ pk$。 Informal Proof至於為什麼當p越大，極值發生位置會越來越前面，簡單的證明如下：當$f(x)$極值發生時的必要條件是$\\nabla f(x) = 0$$\\nabla (f(x) - px) = \\nabla f(x) - \\nabla px = \\nabla f(x) - p = 0$已知$f(x)$是concave，因此 $\\nabla f(x)$ 遞減因此當$p \\uparrow$，$x$必須越來越小。然而，WQS真正困難的部分其實是證明$f(x)$是concave的，很多時候看到題目都會靠直覺猜它是concave，然後直接使用此性質（就像猜某個greedy方式會是最佳解）。 Intuition我覺得WQS二分搜有一個很好的intuition：$f(x)$是某件事做了$x$次後得到的值，$p$就可以當作每做一次所需的成本，當成本($p$)越大，我們就不能做太多次操作（若我們的目標是maximize $f(x)$）。因此！理所當然當$p$越大，$x_0$就會越小。 Application直接來看一題很經典的WQS二分的題目AI-666 賺多少：題目敘述：給定$n$天股票價格，求只能交易$k$次的情況下最多能賺多少錢？（假設最多同時只能持有一張股票）這題有一個很簡單的dp解：$dp0[i][j]$表示在第$i$天，交易了$j$次且當下沒有股票的最佳解$dp1[i][j]$表示在第$i$天，交易了$j$次且當下持有股票的最佳解轉移式：$dp0[i][j] = max(dp0[i-1][j], dp1[i-1][j-1] + a[i])$$dp1[i][j] = max(dp1[i-1][j], dp0[i-1][j] - a[i])$最後答案就是$dp0[n][k]$。但很明顯，此做法的時間、空間複雜度為$O(n^2)$。嘗試將問題轉換成「不限制交易次數」，我們可以發現轉移式就會變成：$dp0[i] = max(dp0[i-1], dp1[i-1] + a[i])$$dp1[i] = max(dp1[i-1], dp0[i-1] - a[i])$其中，$dp0[i]$表示在第$i$天，當下沒有股票的最佳解，$dp1[i]$表示在第$i$天，當下持有股票的最佳解。接著，我們嘗試將每次交易都增加一個$p$元的手續費，此時轉移式變成：$dp0[i] = max(dp0[i-1], dp1[i-1] + a[i])$$dp1[i] = max(dp1[i-1], dp0[i-1] - a[i] - p)$我們可以在$O(n)$的時間解決這個「有手續費且沒有交易次數限制」的問題。最後，我們可以對$p$做二分搜找到某個$p$，使得發生最大值時的交易次數$\\leq k$最終答案就會是此演算法的輸出$+ pk$。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include &lt;bits/stdc++.h&gt;#define int long long#define ll long long#define pb push_back#define sz(x) (x).size()#define all(x) (x).begin(), (x).end()#define fastio cin.tie(0); ios_base::sync_with_stdio(false);using namespace std;const int maxn = 2e6+5;const int mod = 1e9+7; // 998244353;const ll inf = 1ll &lt;&lt; 61;typedef pair&lt;int,int&gt; P;int n, k;int a[maxn];P dp0[maxn], dp1[maxn];void init(){ dp0[0] = P(0, 0); dp1[0] = P(-inf, 0);}P cal(int p){ init(); P res = {0, inf}; for(int i=1;i&lt;=n;i++){ // 沒股票 if(dp0[i-1] &gt; P(dp1[i-1].first + a[i] - p, dp1[i-1].second - 1)) dp0[i] = dp0[i-1]; else dp0[i] = P(dp1[i-1].first + a[i] - p, dp1[i-1].second - 1); // 有股票 if(dp1[i-1] &gt; P(dp0[i-1].first - a[i], dp0[i-1].second)) dp1[i] = dp1[i-1]; else dp1[i] = P(dp0[i-1].first - a[i], dp0[i-1].second); res = max(res, dp0[i]); } return res;}void solve(){ cin &gt;&gt; n &gt;&gt; k; for(int i=1;i&lt;=n;i++) cin &gt;&gt; a[i]; int l = 0, r = 5e4; P res = cal(0); int pp; if(-res.second &lt;= k){ cout &lt;&lt; res.first; return; } while(l &lt;= r){ int mid = (l + r) / 2; res = cal(mid); if(-res.second &gt; k){ pp = mid; l = mid + 1; } else{ r = mid - 1; } } cout &lt;&lt; cal(pp+1).first + (pp+1) * k;}signed main(){fastio int T = 1; //cin &gt;&gt; T; while(T--){ solve(); }} 在TIOJ上要稍微壓一點常數（e.g. 二分搜範圍） WQA二分搜的過程其實有不少細節，請未來的我自行體會。但就結論而言，一個簡單且可以避免錯誤的方法是「找最大的$p$使得最後的交易次數不滿足題目限制」最後我們所需$p$就會是$p+1$(如果是離散的話)。 Related Problems 美食博覽會 (k 值加大版)Subarray SquaresTree IE2. Guard Duty (medium)F. New Year and Handle ChangeE. Gosha is hunting","link":"/2025/01/08/WQS%E4%BA%8C%E5%88%86%E6%90%9C/"},{"title":"Backpropagation 反向傳播","text":"學PyTorch不如自己刻一次Backpropagation。 Opening本篇文章是建立在對Gradient Descent、Neural Network架構有基礎了解的前提下，介紹如何透過Backpropagation來更新神經網路的參數，並且使用NumPy手刻出Backpropagation的過程。 Forward 向前傳播首先，先來看神經網路的基本架構： 這是一個簡易的神經網路，輸入、隱藏層皆有兩個Neurons加上一個Bias，輸出則有一個Neurons。對於某Neuron $X_i$，向前傳播過程可以寫成： $$ \\begin{aligned} Z^{(1)}_j &= \\sum_{i=1}^{D} W^{(1)}_{ij}X_i + B^{(1)}_j \\\\ A^{(1)}_j &= h(Z^{(1)}_j) \\\\ Z^{(2)}_k &= \\sum_{j=1}^{M} W^{(2)}_{jk}A^{(1)}_j + B^{(2)}_k \\\\ A^{(2)}_k &= Z^{(2)}_k \\\\ \\text{E}(W,B) &= \\frac{1}{2 } \\sum_{k=1}^{K} (A^{(2)}_k - Y_k)^2 \\end{aligned} $$ 這裡的$h$表示隱藏層的激活函數（Activation Function），這裡因為以Regression為例，所以輸出層的激活函數為Identity Function。$\\text{E}$表示Loss Function，這裡以Mean Squared Error為例。 若僅考慮Variables/Activations的Dependency，可以寫成： $$ \\begin{aligned} X_i \\rightarrow W^{(1)}_{ij} \\rightarrow Z^{(1)}_j \\rightarrow A^{(1)}_j \\rightarrow W^{(2)}_{jk} \\rightarrow Z^{(2)}_k \\rightarrow A^{(2)}_k \\leftrightarrow Y_k \\rightarrow \\text{E} \\end{aligned} $$ Backward 反向傳播接著，我們要來更新$W^{(2)}$的參數，我們的目標是計算$\\frac{\\partial E}{\\partial W^{(2)}_{jk}}$，但這個東西很難直接算出來，因此我們考慮使用Chain Rule，透過上式的Dependency關係將其拆成 : $$ \\begin{aligned} \\frac{\\partial E}{\\partial W^{(2)}_{jk}} &= \\frac{\\partial E}{\\partial Z^{(2)}_k} \\cdot \\frac{\\partial Z^{(2)}_k}{\\partial W^{(2)}_{jk}} \\\\ \\end{aligned} $$ 透過一些簡單的偏微分計算，我們得到： $$ \\begin{aligned} \\frac{\\partial E}{\\partial Z^{(2)}_k} &= A^{(2)}_k - Y_k = \\delta_{k} \\\\ \\frac{\\partial Z^{(2)}_k}{\\partial W^{(2)}_{jk}} &= A^{(1)}_j \\end{aligned} $$ 接著再次使用Chain Rule，分解$\\frac{\\partial E}{\\partial W^{(1)}_{ij}}$： $$ \\begin{aligned} \\frac{\\partial E}{\\partial W^{(1)}_{ij}} &= \\sum_{k=1}^{K} \\left( \\frac{\\partial E}{\\partial Z^{(2)}_k} \\cdot \\frac{\\partial Z^{(2)}_k}{\\partial Z^{(1)}_j} \\cdot \\frac{\\partial Z^{(1)}_j}{\\partial W^{(1)}_{ij}} \\right) \\\\ \\end{aligned} $$ 再複習一次大一微積分： $$ \\begin{aligned} \\frac{\\partial E}{\\partial Z^{(2)}_k} &= \\delta_{k} \\\\ \\frac{\\partial Z^{(2)}_k}{\\partial Z^{(1)}_j} &= W^{(2)}_{jk} \\cdot h^{'}(Z^{(1)}_j) \\\\ \\frac{\\partial Z^{(1)}_j}{\\partial W^{(1)}_{ij}} &= X_i \\end{aligned} $$ 其中，我們定義第一層的誤差信號 $\\delta_j$ 為： $$ \\begin{aligned} \\delta_j &= \\frac{\\partial E}{\\partial Z^{(1)}_j} = \\sum_{k=1}^{K} W^{(2)}_{jk} \\delta_k h'(Z^{(1)}_j) \\end{aligned} $$ 將上述幾式整理，我們可以得到： $$ \\begin{aligned} \\frac{\\partial E}{\\partial W^{(2)}_{jk}} &= \\delta_{k} \\cdot A^{(1)}_j \\\\ \\frac{\\partial E}{\\partial W^{(1)}_{ij}} &= \\delta_{j} \\cdot X_i \\end{aligned} $$ 對於Bias的Gradient，其實就是： $$ \\begin{aligned} \\frac{\\partial E}{\\partial B^{(2)}_{k}} &= \\delta_{k} \\\\ \\frac{\\partial E}{\\partial B^{(1)}_{j}} &= \\delta_{j} \\end{aligned} $$ 如此一來我們就算完最複雜的部分了，可以進入實作了！ Implementation在開始實作前，我們先釐清各個矩陣的維度： 符號 說明 維度 $X$ 輸入矩陣 (m, n_input) $W^{(1)}$ 第一層的權重矩陣 (n_input, n_hidden) $B^{(1)}$ 第一層的偏差矩陣 (1, n_hidden) $Z^{(1)}$ 第一層的線性組合 (m, n_hidden) $A^{(1)}$ 第一層的激活函數結果 (m, n_hidden) $W^{(2)}$ 第二層的權重矩陣 (n_hidden, n_output) $B^{(2)}$ 第二層的偏差矩陣 (1, n_output) $Z^{(2)}$ 第二層的線性組合 (m, n_output) $A^{(2)}$ 第二層的激活函數結果 (m, n_output) 其中，m為Batch Size，n_input為輸入的特徵數，n_hidden為隱藏層的Neuron數，n_output為輸出的Neuron數。 Forward12345678910111213def forward(self, x): &quot;&quot;&quot; Parameters: x (numpy.ndarray): Input matrix of shape (batch_size, n_input). Returns: numpy.ndarray: Output matrix of shape (batch_size, n_output). &quot;&quot;&quot; self.z1 = np.matmul(x, self.w1) + self.b1 self.a1 = self.ReLU(self.z1) self.z2 = np.matmul(self.a1, self.w2) + self.b2 self.a2 = self.z2 return self.a2 在這裡選用ReLU作為隱藏層的Activation Function。 Backward12345678910111213141516171819202122232425262728def backward(self, X, y, pred, lr): &quot;&quot;&quot; Parameters: X (numpy.ndarray): Input matrix of shape (batch_size, n_input). y (numpy.ndarray): Target matrix of shape (batch_size, n_output). pred (numpy.ndarray): Output matrix of shape (batch_size, n_output). lr (float): Learning rate. &quot;&quot;&quot; # STEP 1 batch_size = X.shape[0] delta_output = 2 * (pred - y) / batch_size # STEP 2 dw2 = np.matmul(self.a1.T, delta_output) db2 = np.sum(delta_output, axis=0, keepdims=True) # STEP 3 delta_hidden = np.matmul(delta_output, self.w2.T) * self.ReLU_derivative(self.z1) # STEP 4 dw1 = np.matmul(X.T, delta_hidden) db1 = np.sum(delta_hidden, axis=0, keepdims=True) # UPDATE self.w1 -= lr * dw1 self.b1 -= lr * db1 self.w2 -= lr * dw2 self.b2 -= lr * db2 這裡的ReLU_derivative是ReLU的導函數（也可以換成其他Activation Function，但記得要differentiable）。$ \\delta_k \\text{加了} \\frac{2}{\\text{batch_size}}$ 常數是因為真正MSE函式應為 np.mean((A - Y) ** 2)，這個mean是指將$\\textbf{所有資料}$的Square Error取平均，而上面的MSE公式則是為了讓$\\textbf{單一資料}$Gradient計算時會比較漂亮，所以乘了一個$\\frac{1}{2}$。當初在寫Backward時一直不懂矩陣到底要怎麼乘，但後來發現只要想清楚上面的公式，再對照好矩陣維度就可以了！ Training再來來寫一個簡單的Training Function：123456789101112131415def train(self, X, y, epochs=1000, lr=0.01, verbose=10): &quot;&quot;&quot; Parameters: X (numpy.ndarray): Input matrix of shape (batch_size, n_input). y (numpy.ndarray): Target matrix of shape (batch_size, n_output). epochs (int): Number of epochs. lr (float): Learning rate. verbose (int): Frequency of printing the loss &quot;&quot;&quot; for epoch in range(epochs): pred = self.forward(X) loss = self.MSE(y, pred) self.backward(X, y, pred, lr) if epoch % verbose == 0: print(f&quot;Epoch {epoch}, Loss: {loss:.6f}&quot;) Conclusion這裡我以Regression為例，但實際上只要將Loss Function、Activation Function換掉，就可以應用在其他任務上。以下是一些常見的任務： Tasks 輸出層 Activation Function Loss Function 回歸（Regression） Identity Function Mean Squared Error 二元分類（Binary Classification） Sigmoid Binary Cross-Entropy 多類分類（Multi-Class Classification） Softmax Categorical Cross-Entropy 再做一些微分時小小的修改就可以了。但我認為重點在於理解Backpropagation中使用Chain Rule的技巧，還有計算Partial Derivative的一些細節。","link":"/2025/02/17/backpropagation/"},{"title":"Revisiting Throughput","text":"This is a note about throughput and latency from the perspective of Software and Hardware. OverviewIn the scenario of analyzing the performance of a DNN model, throughput and latency are two fundamental metrics. While these terms are often understood at a high level, when considering them from both hardware and software perspectives at the same time, there are important factors that are often overlooked. Basic Concepts about Throughput and Latency Throughput is defined as the amount of data that can be processed/completed in a given time period. We can report it as : $$ \\begin{align} \\text{Throughput} &= \\frac{\\text{Number of operations}}{\\text{seconds}} \\tag{1} \\\\ \\end{align} $$ Applications require high throughput when a system involves large-scale data processing, where maximizing the number of processed units per second is critical. For example, real-time video processing, security and surveillance, medical diagnosis, and drug discovery all benefit from high throughput. Latency is the time taken to complete a single operation. We can report it as : $$ \\begin{align} \\text{Latency} &= \\text{Time}_{\\text{output generated}} - \\text{Time}_{\\text{input arrives to a system}} \\tag{2}\\\\ \\end{align} $$ In general, real-time interative applications, such as AR, autonomous navigation and robotics, require low latency to ensure a smooth user experience. A Deeper Look at ThroughputFactors Affecting ThroughputIn model inference perspective, throughput is considered as inference per second. Firstly, we can break it down into two components: $$ \\begin{align} \\frac{\\text{inferences}}{\\text{seconds}} &= \\frac{\\text{operations}}{\\text{seconds}} \\times \\frac{1}{\\frac{\\text{operations}}{\\text{inferences}}} \\tag{3} \\\\ \\end{align} $$ The denominator of second term is dictated by only DNN models , because it is defined as how many operations are required to complete a single inference.For the first term, we can further break it down: $$ \\begin{align} \\frac{\\text{operations}}{\\text{seconds}} = \\left( \\frac{1}{\\frac{\\text{cycles}}{\\text{operations}}} \\times \\frac{\\text{cycles}}{\\text{second}} \\right) \\times \\text{number of PEs} \\times \\text{utilization of PEs} \\tag{4} \\end{align} $$ Here, we consider a system comprised of multiple processing elements (PEs), where PE means a simple or primitive core that performs a single MAC operation. Let’s take a closer look at each term: $\\frac{1}{\\frac{\\text{cycles}}{operation}}$ is IPC (Instructions per Cycle), which is the number of operations that can be completed in a single cycle. $\\frac{\\text{cycles}}{\\text{second}}$ is the clock rate of the system.The combination of these two terms is the peak throughput of the system, which is the maximum number of operations that can be completed in a second. $\\text{number of PEs}$ means the amount of parallelism in the system. $\\text{utilization of PEs}$ is the fraction of time that the PEs are busy performing operations.This term is determined by DNN model(i.e. memory-bound or not). Back to equation (3), we can say that the first term is dictated by DNN hardware and DNN models, while the second term is dictated by the DNN model. Increase ThroughputIf we want to increase throughput from a hardware perspective, we can consider the following: Increase $\\frac{\\text{cycle}}{\\text{second}}$ $\\rightarrow$ higher clock rate $\\rightarrow$ reducing critical path. Affected by design of the MAC. Increase number of PEs $\\rightarrow$ multiple MACs perform in parallel. If area cost of the system is fixed Increasing the area density of the PEs. Trading off on-chip storage area for more PEs. Increasing the density of PEs Reducing the logic associated with delivering operands to a MAC. Multiple MACs share control logic. $\\text{utilization of PEs} = \\frac{\\text{number of active PEs}}{\\text{number of PEs}} \\times \\text{utilization of active PEs}$First term means ability to distribute the workload to PEs. (在做事的PE/所有PE)Second term means how efficiently those active PEs are processing the workload. (做有用的事情的PE) Effectual and Ineffectual OpertaionsIf we consider effectual and ineffectual operations, we can further break down the first term of equation (4) as follows: $$ \\begin{align} \\frac{\\text{operations}}{\\text{cycle}} = \\frac{\\text{EO} + \\text{UIO}}{\\text{cycle}} \\times \\frac{\\text{EO}}{\\text{EO} + \\text{UIO}} \\times \\frac{1}{\\frac{\\text{EO}}{\\text{operations}}} \\tag{5} \\end{align} $$ where $\\textbf{EO}$ is effectual operations and $\\textbf{UIO}$ is unexploited ineffectual operations(e.g. MAC accumulates anything multiplied by zero $\\rightarrow$ $\\color{red}{\\text{sparsity}}$).In equation (5), the first term is a constant for a given hardware accelerator design, the second term is the ability of the hardware to exploit the ineffectual operations(硬體發覺無效計算的能力), and the last term is related to amount of sparsity and depends on the DNN model. Reducing Precision (e.g. Quantization)Lowering precision of computation, such as through quantization is a common optimization technique. Lower precision means fewer bits to process per operation, which reduces the bandwidth requirements. This allows for higher utilization of PEs, ultimately leading to increased throughput(i.e. more operations per second).However, supporting multiple levels of precision require additional hardware, which may increase the critical path and reduces the area density of PEs. This results in a decrease in overall throughput. Factors that Affect Inference per SecondBased on the above discussion, we can summarize the factors that affect inference per second as this table: Factor Hardware DNN Models Input Data Operations per inference ✓ Operations per cycle ✓ Cycles per second ✓ Number of PEs ✓ Number of active PEs ✓ ✓ Utilization of active PEs ✓ ✓ Effectual operations out of (total) operations ✓ ✓ Effectual operations plus unexploited ineffectual operations per cycle ✓ Reference Course slides of Edge AI, NYCU.","link":"/2025/03/20/throughput/"},{"title":"我的第一篇貼文","text":"這是一篇測試用的文章。 首先先來測試一些基本的文字嗨，你好。I am Chung Pang Chun, and the handle I often use is bonginn.1 + 1 = 2 測試數學式$x^n + y^n = z^n$ \\begin{equation} \\begin{aligned} \\frac{-b + \\sqrt{b^2 - 4ac}}{2a} \\end{aligned}\\end{equation} 測試code12345678#include &lt;iostream&gt;using namespace std;int main(){ int a, b; cin &gt;&gt; a &gt;&gt; b; cout &lt;&lt; a + b;} 測試圖片 測試連結codeforces","link":"/2024/08/24/test/"},{"title":"TIOJ 2070","text":"Solution of TIOJ 2070 - Special Judge。 Description給你由小寫英文字母組成的字串$X$，請判斷是不是每一個前$K$個英文字母的子集都是$X$的子序列。 Observation 1「前$k$個英文字母的『子集』的排列」都是子序列，其實就相當於要你判斷「是否前$k$個英文字母的的排列」都是子序列（abc的所有排列都是，那a, b, c, ab, ac, bc, abc也當然都會是）。 Observation 2範圍：$K \\le 20$ $\\rightarrow$位元DP！ Solution定義dp[S] = 當狀態為S時，字串所需的最短長度能夠使得滿足題目的要求，當dp[S]為n+1時，表示狀態S無法滿足。另外定義pos[i][j]表示在位置j往右出現字元i最近的位置，-1表示右邊找不到i。轉移式：$dp[i + (1 &lt;&lt; j)] = max(dp[i + (1 &lt;&lt; j)], pos[j][dp[i]]);$轉移過程中只要有出現了任何一個pos = -1就輸出$No$，否則就輸出$Yes$。 Code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;bits/stdc++.h&gt;#define int long long#define pb push_back#define all(x) (x).begin(), (x).end()#define fastio cin.tie(0); ios_base::sync_with_stdio(false);#define INF 1e15+9using namespace std;const int maxn = 2e5+5;const int MOD = 1e9+7; // 998244353;typedef pair&lt;int,int&gt; P;void solve(){ int k; string s; cin &gt;&gt; k &gt;&gt; s; char myChar[1005] = {}; for(int i=1;i&lt;=s.size();i++) myChar[i] = s[i-1]; int pos[50][1001]; for(int i=0;i&lt;k;i++){ int curPos = -1; for(int j=s.size();j&gt;=0;j--){ pos[i][j] = curPos; if(myChar[j] - 'a' == i) curPos = j; } } int S = (1 &lt;&lt; k); int dp[S+5] = {}; // dp[s] = 狀態s時, 會往右延伸到哪裡, INF表沒辦法 for(int i=0;i&lt;S;i++){ for(int j=0;j&lt;k;j++){ // 看看哪些沒在狀態i裡面 if((i &gt;&gt; j) % 2 == 0){ int nearest = pos[j][dp[i]]; if(nearest == -1){ cout &lt;&lt; &quot;No&quot; &lt;&lt; '\\n'; return; } dp[i + (1 &lt;&lt; j)] = max(dp[i + (1 &lt;&lt; j)], pos[j][dp[i]]); } } } cout &lt;&lt; &quot;Yes&quot; &lt;&lt; '\\n';}signed main(){fastio int T = 1; cin &gt;&gt; T; while(T--){ solve(); }} Bonus這一題的前身是「構造一個最短字串滿足此性質」，此題是一個co-NP-complete的問題目前能找到的最短且有系統的構造方法如下：12345678for(int i=1;i&lt;=k * k - 2 * k + 4;i++){ if(i &lt;= k) cout &lt;&lt; (char)('a' + i - 1); else if(i &gt; k &amp;&amp; (i - 2) % (k - 1) == 0) cout &lt;&lt; 'a'; else cout &lt;&lt; (char)((int)floor((i - 1) * (k - 2) / (k - 1)) % (k - 1) + 2 + 'a' - 1);}長度：$k^2 - 2k + 4$","link":"/2024/08/24/tioj2070/"},{"title":"115 交大資工所丙組推甄心得","text":"這篇文章沒有面試過程！ 前言想了很久要不要寫這篇文，除了我認為我有點幸運到不值得參考、放榜結果神奇到不可思議外，更多的是我覺得有很多更厲害的人值得丙組正（逕）取的位置。但準備推甄資料的過程中受到太多人的幫助，抱持著善的循環的心，決定還是將自己的準備過程及心得記錄下來給學弟妹參考，也順便分享給未來的自己看。 丙組是什麼？交大資工系碩士班分成以下幾個所： 資訊科學與工程研究所（資科工所） 網路工程研究所（網工所） 多媒體工程研究所（多工所） 數據科學與工程研究所 資訊安全研究所 基本上以上每個所都是$\\textbf{正所}$（雖然有些人會認為資科工才是最正的所），其中在推甄入學時，資科工所又分為甲組和丙丁戊組，甲組和網工、多工所一起算在資訊聯招中，丙丁戊則是獨立的管道，推甄時與其他組最大的不同是過了書審後$\\textbf{需要面試}$。丙組是實作導向的組別，簡單說就是$\\textbf{不太看成績，以實作能力為主}$，適合那些成績普通但有實作能力的高手們；丁戊組分別是系統與網路管理實務（系計中）和校務資訊系統技術與實務（校計中）。丙組在入學後與資甲並沒有太大的差異，都會是資科工所，唯一的差別是可以找的教授名單不同，且教授在丙組的名額也會比較少（基本上是一個）。 可以到這裡看更多關於丙組的介紹。 背景 校系：交大資工系 成績：$53.37\\%$ (GPA $3.84 / 4.30$) 專題：LLMs 量化演算法的研究 國科會大專生研究計畫 ICPC Regional 銅牌$\\times$1、TOPC 銀牌$\\times$2 CPE Expert Level 6題 ($1.7\\%$) 我沒有任何論文發表和業界實習的經驗，在校成績可以也可以用慘來形容，除了一點點的演算法競賽成績和大專生計畫之外，其實與一般人沒什麼太大的差別。 推甄準備資料自傳我的自傳分成四大部分：個人簡介、專題作品、學術研究、競賽與其他經驗 個人簡介我大概花了$\\frac{1}{4}$到$\\frac{1}{3}$頁介紹自己，簡單講自己有興趣的研究領域、對資工的熱愛以及個人特質。這部分對於申請資工所的自傳來說不是重點，因此不必花過多的篇幅，教授應該也不太會看。 專題作品我認為這部分是自傳中最重要的部分，這也是你最可以展現自己專業的地方。在這部分我列了三個專題作品，每個專題又會花大約$100$到$150$字描述，其中也包含我的畢業專題。我認為在這部分要盡可能地展現出「可量化的實際數據」，寫「我提升了模型的推論速度」不如寫「我將模型的推論速度提升$2$倍，同時降低$50\\%$記憶體使用量」，同時也可以說明使用到了什麼技術、專題規模等，如果有GitHub連結也可以附上。 學術研究我在這部分放了我的國科會大專生研究計畫，簡單描述了我的計劃內容，研究動機，預期成果等，但計畫還在進行中，我就沒有花太多篇幅寫了。如果你有發表過論文，我認為在這部分可以花多一點篇幅，$\\textbf{我相信丙組教授對於論文發表一定會加非常多分}$。 競賽與其他經驗主要放了我比過的演算法競賽成績和我在PCCA的經歷。 至於版面配置的部分，我是使用Canva簡單製作，$\\textbf{共兩頁A4大小}$。我認為只要字不要太小太大，版面不要太雜亂，讓人看得舒服就可以了。 簡歷在我給我的導師看過我的自傳草稿後，他給我的回饋： Btw, I feel you have a very strong record. If you can have a CV summarizing your achievements as bullets, that will be much better. My two cents 雖然交大資工所沒有叫你附上簡歷，但我認為在自傳的前一頁附上1-page CV 非常重要，據我的導師表示，他審聯招的自傳基本上只看第一頁，所以你用心寫的精美自傳教授可能只會花不到一分鐘看，這時候在第一頁就附上CV可以讓教授更快了解你的能力和特質。雖然說在丙組這種情況應該比較不會發生，但還是建議大家附上。 我的CV基本上就是把我的自傳的內容精簡成一頁的Bullet Points，每個Project只列出兩三點最重要的成果，盡可能地可以讓人在一分鐘內看完的同時還能夠大致了解你的專業能力。 讀書計劃我發現網路上大多數人都會說交大的讀書計劃寫了教授也不會看，看了也不會信，但我認為在丙組這種實作導向的組別，$\\textbf{讀書計劃更大的意義是展現出你對於自己有興趣的研究領域的掌握程度和未來規劃}$，好的讀書計劃可以讓教授更相信你對於該領域的了解和熱忱。 至於什麼是好的讀書計劃，根據我的經驗，要盡可能地寫的$\\textbf{具體}$一點，避免寫一些空泛的東西，例如：「入學前加強英文能力」、「學習xx領域相關知識」我認為就過於籠統；相對地，若能具體點出要學習的工具或語言、要貢獻的開源專案或是預計專研的技術細節，會讓整份讀書計畫更有說服力，也能展現你對該領域的現況有足夠的認知。 推薦信交大資工所規定需要至少兩封推薦信（上限是五封），如果你能找到五個人幫你寫推薦信當然最好，但我比較熟的教授就只有我的導師和專題教授，所以只有兩封。我的做法是在寄信詢問教授意願時會先附上我的自傳草稿，讓教授對你有基本的認識，教授答應後通常會要你提供一份推薦信的草稿，基本上就是以教授的視角來推薦自己，可以具體寫一些過去曾經在該教授指導下做過的專案、修過的課、合作過的計劃等等，格式大概會像是： 本人在此誠摯推薦本系四年級學生XXX參加貴所的研究所推甄。在我過去所教授的YYY課程中，XXX展現出優異的學習態度與實作能力，並完成了ZZZ專案，成果不僅在效率上的表現相當突出，在與其他組員的合作上也展現了高度的溝通、協作能力，充分顯示出其卓越的研發與實作能力。此外，XXX同學在本人的指導下於大三時完成專題研究「WWW」，他能獨立完成文獻整理、實驗設計與結果分析，展現了扎實的研究能力與批判思維。… (其他事蹟)我深信，XXX同學兼具學術潛力、研究能力與團隊合作精神。若能進入貴所繼續深造，必定能做出卓越的學術與實務貢獻。 建議要早一點寄信給教授，可以的話就在暑假（七八月）就寄信詢問教授的意願，我是大概九月中才寄信給教授請他幫我寫推薦信，而我的專題教授是一個非常會趕Deadline的人，導致我是在推薦信上傳截止前一天凌晨四點才拿到我的第二封推薦信，非常刺激…，但還是非常感謝他們。 推甄結果最後我的丙組推甄結果是$\\textbf{逕取}$，也就是不用面試就可以直接錄取。丙丁戊組的逕取生應該可以算是第一批上岸的，因為丙組會在交大第一梯次放榜前的約兩個禮拜先公布初試通過名單，而逕取生被會同時收到直接錄取的電話通知。逕取最大的優點就是可以提早找指導教授，因為丙組的教授名單比較少，且每個教授名額只有一個，因此在丙組的熱門教授會非常搶手，而我也很幸運的被我的專題指導教授用那寶貴的一個名額收留。 除了交大資工所丙組之外，我也申請了交大資訊聯招、數據所、清大資工所甲組、資應所、台大資工所、資工所人工智慧碩士班、網媒所，附上我的其他組的結果： 學校 組別 結果 交大 資丙 $\\textbf{逕取}$ 交大 資甲（資訊聯招） 備取207 交大 網工所（資訊聯招） 備取138 交大 多工所（資訊聯招） 備取157 交大 數據所 備取130 清大 資工所甲組 落榜 清大 資應所 初試不通過 台大 資工所 落榜 台大 人工智慧碩士班 落榜 台大 網媒所 落榜 依我的成績台大全落榜很正常，但清大資工甲落榜和資應所沒過初試有點非常意外，果然所有需要看成績的組別都不適合我這種不會讀書的人… 導師的推甄審查標準前陣子跟我的導師吃導聚的時候他有跟我分享了他審推甄資料的標準，雖然他是負責資訊聯招的，但還是記錄一下給大家參考： 備審基本上只看一頁，因此務必在第一頁附上1-page CV。 會先用校名分組，台大交大一組、清大一組、成大一組、中字一組、其他，接著每個組會有一個GPA/$\\%$數 $\\rightarrow$ 基本審查分數的Mapping。 大部分是校系名+$\\%$數決定，但如果有以下幾點會另外加分： 發表論文：加大分 國科會大專生研究計畫：加大分 到中研院跟教授／院士實習：加分 競賽（ICPC、資安競賽、CPE）：加分 業界實習：加一點點分 沒什麼在看推薦信，除非是教授「親自」幫你寫，而且寫得很好。 如果成績不好，基本上專題是唯一翻身機會。 其中我覺得CP值最高的絕對是$\\textbf{國科會大專生研究計畫}$，在教授的角度，申請這份計劃並通過或許就代表了你有一定的學術潛力和熱忱，而大專生研究計畫我覺得只要有心認真寫計畫書，且題目有一定的價值，通常都可以通過。如果有空可以再來寫一篇計畫書撰寫的心得。 當然，每個教授的推甄審查標準非常不一樣，但可以肯定的是，$\\textbf{顧好成績絕對是最安全的選擇}$。雖然近年資工成績內卷的非常嚴重，但我認為很大一部分的原因也是因為碩班推甄審查標準的不確定性而盲目內卷，如果標準公開一致，且成績不再是唯一，我覺得站在學生的角度來說或許會是一件好事。 在這裡也順便分享我看到的兩個教授對於推甄審查的一些看法和建議，我覺得說得很好： 劉育綸教授 吳凱強教授 結語我自認我推甄的整個過程真的非常非常幸運，我從最一開始就決定絕對不要考試，因為我覺得考碩班是一件非常浪費生命的事情，因此全心全力沒有退路地投入在推甄的準備。交大有專門開了實作導向的組別、我的專題教授剛好也在丙組教授名單中、直接逕取不用承受面試的壓力與不確定性、教授願意收留我，這些事情如果有任何一個沒有發生，我不知道我現在會在做什麼。總之還是很感謝當初幫助我的所有人，也祝大家順利！","link":"/2025/11/20/%E4%B8%99%E7%B5%84%E5%BF%83%E5%BE%97/"},{"title":"Roofline Model for Performance Analysis","text":"A note about Roofline Model. Definition FLOPs : Number of Floating Point Operations. A Multiply-Add operation(MAC) is counted as 2 FLOPs. FLOPS : Floating Point Operations Per Second. $$ \\begin{aligned} \\text{FLOPS} &= \\frac{\\text{FLOPs}}{\\text{Second}} \\\\ \\end{aligned} $$ OPs : Number of Operations. (Not necessarily Floating Point Operations) OPS : Number of Operations Per Second. $$ \\begin{aligned} \\text{OPS} &= \\frac{\\text{OPs}}{\\text{Second}} \\\\ \\end{aligned} $$ Memory Bandwidth : The rate at which data can be read from or written to memory (Bytes per second). Arithmetic Intensity : The ratio of Total OPs performed to the Total Bytes moved. $$ \\begin{aligned} \\text{I} &= \\frac{C \\text{(Computations, OPs)}}{M \\text{(Memory Access, Bytes)}} \\\\ \\end{aligned} $$ Process Of DNN Inference on HardwareStep 1 (Memory Access): Moving Input/Weghts from memory to CPU/GPU.Step 2 (Compute): Perform the operations(e.g. Linear, Convolution…).Step 3 (Memory Access): Writing the computed activation back to memory.The process can be visualized as below: Therefore, evaluating performance requires simultaneous consideration of memory bandwidth and processing unit capabilities. If a layer involes extensive computation but minimal memory access, it is termed a computation bottleneck. Conversely, if a layer requires frequent memory access but minimal computation, it is termed a memory bottleneck. We can clearly distinguish between these two scenarios according to the Roofline Model. Roofline Model Plot the Roofline ModelFirstly, we need to determine the Peak Computational Performance (operations per second, OPS) $\\pi$ and Peak Memory Bandwidth(bytes per second) $\\beta$ specific to the target hardware device. Create a graph with performance (OPS) on the y-axis and arithmetic intensity (OPs/byte) on the x-axis: Draw a horizontal line equal to the peak computational performance, representing the maximum achievable performance by hardware. Draw a diagonal line from the original point with a slope equal to the peak memory bandwidth, representing the maximum memory bandwidth avaible on the system. After plotting these two lines, we can get Roofline Model as below: Analyze Performance for LayersFor each layer in LLMs, we can calculate its arithmetic intensity(OPs/byte) by dividing the required operations(OPs) by the amount of data transferred(bytes). According to the Roofline Model just plotted, the theoretical maximum performance for each layer is determined by the position on the graph corresponding to the arithmetic intensity of the layer. It allows us to ascertain whether this layer is compute-bound or memory-bound: If the layer’s arithmetic intensity is below the turning point, it means that the computational workload per memory access is low. Even saturating the memory bandwidth, it does not utilize the full computational capability of the hardware. In this case, the layer is constrained by memory access, and it is termed memory-bound. If the layer is memory-bound, we can optimize the performance by quantization, kernel fusion or increasing the batch size(decrease the number of memory access). Conversely, if the layer’s arithmetic intensity is above the turning point, it means that the computational workload per memory access is high. It implies that the layer requires only a small amount of memory access to consume a significant amount of computational capability, and it is termed compute-bound. Estimated Execution TimeThe execution time is the number of operations divided by performance $$ \\begin{aligned} \\text{T} &= \\frac{C}{P} = \\frac{C}{\\min(\\pi, \\beta \\cdot I)} \\\\ \\end{aligned} $$ Memory Bound: $$ \\begin{aligned} \\text{T} &= \\frac{C}{\\beta \\cdot I} = \\frac{C}{\\beta \\cdot \\frac{C}{M}} = \\frac{M}{\\beta}\\\\ \\end{aligned} $$ Compute Bound: $$ \\begin{aligned} \\text{T} &= \\frac{C}{\\pi} \\\\ \\end{aligned} $$ Example Usage of Roofline ModelHere, I provide an example of using the Roofline Model to analyze the performance of a specific layer in a neural network. Suppose we have some operations with #FLOPs and #Memory Access as below: Operation #FLOPs (M) #Memory Access (MB) op1 57.8M 25MB op2 51.4M 2.3MB op3 925M 25MB op4 236M 3.7MB op5 172M 1.3MB op6 231M 1.3MB Reduce #Memory Access when it’s Memory Bound (op1 vs op2) Reducing #FLOPs results no speed up. (refer op1 and op3) Higher computational capability(higher $\\pi$) does not improve performance. Operation #FLOPs (M) #Memory Access (MB) Operational Intensity Attainable Performance (GFLOPS) Theoretical Inference Time op1 57.8M 24.5MB 2.4 226.5 255 op2 51.4M 2.3MB 22.5 2156.7 23.8 op3 925M 24.5MB 37.7 3622.6 255 Reduce #FLOPs when it’s Compute Bound (op5 vs op6) Reducing #Memory Access results no speed up. (refer op4 and op6) Operation #FLOPs (M) #Memory Access (MB) Operational Intensity Attainable Performance (GFLOPS) Theoretical Inference Time op4 236M 3.7MB 64 4608 51.2 op5 172M 1.3MB 132 4608 37.3 op6 231M 1.3MB 174 4608 50.3 From these two scenarios, we can observe that smaller memory access or smaller FLOPs will not necessarily lead to better performance. ConclusionDuring the user’s interative with LLMs, the prefill stage excutes only once, while the decode stage is repeatedly performed to progressively generate the output. Moreover, when calculating the arithmetic intensity of each layer in LLMs, we will observe that the Prefill stage is almost entirely compute-bound, while the Decode stage is memory-bound. Therefore, optimizing for the memory bound characteristics of the decode stage becomes crucial for enhancing the inference performance of LLMs. Reference LLM Inference Unveiled: Survey and Roofline Model Insights Course slides of Edge AI, NYCU.","link":"/2025/03/09/roofline/"}],"tags":[{"name":"code","slug":"code","link":"/tags/code/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"AI Efficiency","slug":"AI-Efficiency","link":"/tags/AI-Efficiency/"},{"name":"Misc","slug":"Misc","link":"/tags/Misc/"}],"categories":[{"name":"Note","slug":"Note","link":"/categories/Note/"},{"name":"Solution","slug":"Solution","link":"/categories/Solution/"},{"name":"Misc","slug":"Misc","link":"/categories/Misc/"}],"pages":[{"title":"About Me","text":"My name is Pang-Chun, Chung, and I am currently a junior at Department of Computer Science, National Yang Ming Chiao Tung University (NYCU). I’m interested in competitive programming, machine learning, and deep learning. I hope I can share what I have learned with my future self through this blog.If you have any questions regarding this blog posts, feel free to contact me via this email!","link":"/about/index.html"}]}