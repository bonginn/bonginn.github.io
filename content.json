{"posts":[{"title":"WQS 二分搜","text":"一篇WQS二分搜的筆記，希望能讓自己不要這麼快忘記的個優化技巧。 IntroductionWQS 二分搜又稱Aliens優化，是一個可以將二維的DP問題轉化成一個維度的優化技巧複雜度可以從$O(N^2)$下降到$O(NlogC)$，但題目本身需要一些特性。已知某函數$f(x)$為concave function，且我們有某種演算法可以在線性時間內求出$f(x) - px$的最大值及最大值發生的位置($x_0$)。接下來，我們畫出$p=0 - 5$的Cases :可以發現，$x_0$隨著$p$上升逐漸下降！因此若我們希望找到$f(x)$的最大值且$x&lt;=k$，我們可以對$p$做二分搜，找到最近的$p$使得$f(x) - px$的$x_0$$\\le k$，最後答案就會是演算法的輸出$+ pk$。 Informal Proof至於為什麼當p越大，極值發生位置會越來越前面，簡單的證明如下：當$f(x)$極值發生時的必要條件是$\\nabla f(x) = 0$$\\nabla (f(x) - px) = \\nabla f(x) - \\nabla px = \\nabla f(x) - p = 0$已知$f(x)$是concave，因此 $\\nabla f(x)$ 遞減因此當$p \\uparrow$，$x$必須越來越小。然而，WQS真正困難的部分其實是證明$f(x)$是concave的，很多時候看到題目都會靠直覺猜它是concave，然後直接使用此性質（就像猜某個greedy方式會是最佳解）。 Intuition我覺得WQS二分搜有一個很好的intuition：$f(x)$是某件事做了$x$次後得到的值，$p$就可以當作每做一次所需的成本，當成本($p$)越大，我們就不能做太多次操作（若我們的目標是maximize $f(x)$）。因此！理所當然當$p$越大，$x_0$就會越小。 Application直接來看一題很經典的WQS二分的題目AI-666 賺多少：題目敘述：給定$n$天股票價格，求只能交易$k$次的情況下最多能賺多少錢？（假設最多同時只能持有一張股票）這題有一個很簡單的dp解：$dp0[i][j]$表示在第$i$天，交易了$j$次且當下沒有股票的最佳解$dp1[i][j]$表示在第$i$天，交易了$j$次且當下持有股票的最佳解轉移式：$dp0[i][j] = max(dp0[i-1][j], dp1[i-1][j-1] + a[i])$$dp1[i][j] = max(dp1[i-1][j], dp0[i-1][j] - a[i])$最後答案就是$dp0[n][k]$。但很明顯，此做法的時間、空間複雜度為$O(n^2)$。嘗試將問題轉換成「不限制交易次數」，我們可以發現轉移式就會變成：$dp0[i] = max(dp0[i-1], dp1[i-1] + a[i])$$dp1[i] = max(dp1[i-1], dp0[i-1] - a[i])$其中，$dp0[i]$表示在第$i$天，當下沒有股票的最佳解，$dp1[i]$表示在第$i$天，當下持有股票的最佳解。接著，我們嘗試將每次交易都增加一個$p$元的手續費，此時轉移式變成：$dp0[i] = max(dp0[i-1], dp1[i-1] + a[i])$$dp1[i] = max(dp1[i-1], dp0[i-1] - a[i] - p)$我們可以在$O(n)$的時間解決這個「有手續費且沒有交易次數限制」的問題。最後，我們可以對$p$做二分搜找到某個$p$，使得發生最大值時的交易次數$\\leq k$最終答案就會是此演算法的輸出$+ pk$。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include &lt;bits/stdc++.h&gt;#define int long long#define ll long long#define pb push_back#define sz(x) (x).size()#define all(x) (x).begin(), (x).end()#define fastio cin.tie(0); ios_base::sync_with_stdio(false);using namespace std;const int maxn = 2e6+5;const int mod = 1e9+7; // 998244353;const ll inf = 1ll &lt;&lt; 61;typedef pair&lt;int,int&gt; P;int n, k;int a[maxn];P dp0[maxn], dp1[maxn];void init(){ dp0[0] = P(0, 0); dp1[0] = P(-inf, 0);}P cal(int p){ init(); P res = {0, inf}; for(int i=1;i&lt;=n;i++){ // 沒股票 if(dp0[i-1] &gt; P(dp1[i-1].first + a[i] - p, dp1[i-1].second - 1)) dp0[i] = dp0[i-1]; else dp0[i] = P(dp1[i-1].first + a[i] - p, dp1[i-1].second - 1); // 有股票 if(dp1[i-1] &gt; P(dp0[i-1].first - a[i], dp0[i-1].second)) dp1[i] = dp1[i-1]; else dp1[i] = P(dp0[i-1].first - a[i], dp0[i-1].second); res = max(res, dp0[i]); } return res;}void solve(){ cin &gt;&gt; n &gt;&gt; k; for(int i=1;i&lt;=n;i++) cin &gt;&gt; a[i]; int l = 0, r = 5e4; P res = cal(0); int pp; if(-res.second &lt;= k){ cout &lt;&lt; res.first; return; } while(l &lt;= r){ int mid = (l + r) / 2; res = cal(mid); if(-res.second &gt; k){ pp = mid; l = mid + 1; } else{ r = mid - 1; } } cout &lt;&lt; cal(pp+1).first + (pp+1) * k;}signed main(){fastio int T = 1; //cin &gt;&gt; T; while(T--){ solve(); }} 在TIOJ上要稍微壓一點常數（e.g. 二分搜範圍） WQA二分搜的過程其實有不少細節，請未來的我自行體會。但就結論而言，一個簡單且可以避免錯誤的方法是「找最大的$p$使得最後的交易次數不滿足題目限制」最後我們所需$p$就會是$p+1$(如果是離散的話)。 Related Problems 美食博覽會 (k 值加大版)Subarray SquaresTree IE2. Guard Duty (medium)F. New Year and Handle ChangeE. Gosha is hunting","link":"/2025/01/08/WQS%E4%BA%8C%E5%88%86%E6%90%9C/"},{"title":"Backpropagation 反向傳播","text":"學PyTorch不如自己刻一次Backpropagation。 Opening本篇文章是建立在對Gradient Descent、Neural Network架構有基礎了解的前提下，介紹如何透過Backpropagation來更新神經網路的參數，並且使用NumPy手刻出Backpropagation的過程。 Forward 向前傳播首先，先來看神經網路的基本架構： 這是一個簡易的神經網路，輸入、隱藏層皆有兩個Neurons加上一個Bias，輸出則有一個Neurons。對於某Neuron $X_i$，向前傳播過程可以寫成： $$ \\begin{aligned} Z^{(1)}_j &= \\sum_{i=1}^{D} W^{(1)}_{ij}X_i + B^{(1)}_j \\\\ A^{(1)}_j &= h(Z^{(1)}_j) \\\\ Z^{(2)}_k &= \\sum_{j=1}^{M} W^{(2)}_{jk}A^{(1)}_j + B^{(2)}_k \\\\ A^{(2)}_k &= Z^{(2)}_k \\\\ \\text{E}(W,B) &= \\frac{1}{2 } \\sum_{k=1}^{K} (A^{(2)}_k - Y_k)^2 \\end{aligned} $$ 這裡的$h$表示隱藏層的激活函數（Activation Function），這裡因為以Regression為例，所以輸出層的激活函數為Identity Function。$\\text{E}$表示Loss Function，這裡以Mean Squared Error為例。 若僅考慮Variables/Activations的Dependency，可以寫成： $$ \\begin{aligned} X_i \\rightarrow W^{(1)}_{ij} \\rightarrow Z^{(1)}_j \\rightarrow A^{(1)}_j \\rightarrow W^{(2)}_{jk} \\rightarrow Z^{(2)}_k \\rightarrow A^{(2)}_k \\leftrightarrow Y_k \\rightarrow \\text{E} \\end{aligned} $$ Backward 反向傳播接著，我們要來更新$W^{(2)}$的參數，我們的目標是計算$\\frac{\\partial E}{\\partial W^{(2)}_{jk}}$，但這個東西很難直接算出來，因此我們考慮使用Chain Rule，透過上式的Dependency關係將其拆成 : $$ \\begin{aligned} \\frac{\\partial E}{\\partial W^{(2)}_{jk}} &= \\frac{\\partial E}{\\partial Z^{(2)}_k} \\cdot \\frac{\\partial Z^{(2)}_k}{\\partial W^{(2)}_{jk}} \\\\ \\end{aligned} $$ 透過一些簡單的偏微分計算，我們得到： $$ \\begin{aligned} \\frac{\\partial E}{\\partial Z^{(2)}_k} &= A^{(2)}_k - Y_k = \\delta_{k} \\\\ \\frac{\\partial Z^{(2)}_k}{\\partial W^{(2)}_{jk}} &= A^{(1)}_j \\end{aligned} $$ 接著再次使用Chain Rule，分解$\\frac{\\partial E}{\\partial W^{(1)}_{ij}}$： $$ \\begin{aligned} \\frac{\\partial E}{\\partial W^{(1)}_{ij}} &= \\sum_{k=1}^{K} \\left( \\frac{\\partial E}{\\partial Z^{(2)}_k} \\cdot \\frac{\\partial Z^{(2)}_k}{\\partial Z^{(1)}_j} \\cdot \\frac{\\partial Z^{(1)}_j}{\\partial W^{(1)}_{ij}} \\right) \\\\ \\end{aligned} $$ 再複習一次大一微積分： $$ \\begin{aligned} \\frac{\\partial E}{\\partial Z^{(2)}_k} &= \\delta_{k} \\\\ \\frac{\\partial Z^{(2)}_k}{\\partial Z^{(1)}_j} &= W^{(2)}_{jk} \\cdot h^{'}(Z^{(1)}_j) \\\\ \\frac{\\partial Z^{(1)}_j}{\\partial W^{(1)}_{ij}} &= X_i \\end{aligned} $$ 其中，我們定義第一層的誤差信號 $\\delta_j$ 為： $$ \\begin{aligned} \\delta_j &= \\frac{\\partial E}{\\partial Z^{(1)}_j} = \\sum_{k=1}^{K} W^{(2)}_{jk} \\delta_k h'(Z^{(1)}_j) \\end{aligned} $$ 將上述幾式整理，我們可以得到： $$ \\begin{aligned} \\frac{\\partial E}{\\partial W^{(2)}_{jk}} &= \\delta_{k} \\cdot A^{(1)}_j \\\\ \\frac{\\partial E}{\\partial W^{(1)}_{ij}} &= \\delta_{j} \\cdot X_i \\end{aligned} $$ 對於Bias的Gradient，其實就是： $$ \\begin{aligned} \\frac{\\partial E}{\\partial B^{(2)}_{k}} &= \\delta_{k} \\\\ \\frac{\\partial E}{\\partial B^{(1)}_{j}} &= \\delta_{j} \\end{aligned} $$ 如此一來我們就算完最複雜的部分了，可以進入實作了！ Implementation在開始實作前，我們先釐清各個矩陣的維度： 符號 說明 維度 $X$ 輸入矩陣 (m, n_input) $W^{(1)}$ 第一層的權重矩陣 (n_input, n_hidden) $B^{(1)}$ 第一層的偏差矩陣 (1, n_hidden) $Z^{(1)}$ 第一層的線性組合 (m, n_hidden) $A^{(1)}$ 第一層的激活函數結果 (m, n_hidden) $W^{(2)}$ 第二層的權重矩陣 (n_hidden, n_output) $B^{(2)}$ 第二層的偏差矩陣 (1, n_output) $Z^{(2)}$ 第二層的線性組合 (m, n_output) $A^{(2)}$ 第二層的激活函數結果 (m, n_output) 其中，m為Batch Size，n_input為輸入的特徵數，n_hidden為隱藏層的Neuron數，n_output為輸出的Neuron數。 Forward12345678910111213def forward(self, x): &quot;&quot;&quot; Parameters: x (numpy.ndarray): Input matrix of shape (batch_size, n_input). Returns: numpy.ndarray: Output matrix of shape (batch_size, n_output). &quot;&quot;&quot; self.z1 = np.matmul(x, self.w1) + self.b1 self.a1 = self.ReLU(self.z1) self.z2 = np.matmul(self.a1, self.w2) + self.b2 self.a2 = self.z2 return self.a2 在這裡選用ReLU作為隱藏層的Activation Function。 Backward12345678910111213141516171819202122232425262728def backward(self, X, y, pred, lr): &quot;&quot;&quot; Parameters: X (numpy.ndarray): Input matrix of shape (batch_size, n_input). y (numpy.ndarray): Target matrix of shape (batch_size, n_output). pred (numpy.ndarray): Output matrix of shape (batch_size, n_output). lr (float): Learning rate. &quot;&quot;&quot; # STEP 1 batch_size = X.shape[0] delta_output = 2 * (pred - y) / batch_size # STEP 2 dw2 = np.matmul(self.a1.T, delta_output) db2 = np.sum(delta_output, axis=0, keepdims=True) # STEP 3 delta_hidden = np.matmul(delta_output, self.w2.T) * self.ReLU_derivative(self.z1) # STEP 4 dw1 = np.matmul(X.T, delta_hidden) db1 = np.sum(delta_hidden, axis=0, keepdims=True) # UPDATE self.w1 -= lr * dw1 self.b1 -= lr * db1 self.w2 -= lr * dw2 self.b2 -= lr * db2 這裡的ReLU_derivative是ReLU的導函數（也可以換成其他Activation Function，但記得要differentiable）。$ \\delta_k \\text{加了} \\frac{2}{\\text{batch_size}}$ 常數是因為真正MSE函式應為 np.mean((A - Y) ** 2)，這個mean是指將$\\textbf{所有資料}$的Square Error取平均，而上面的MSE公式則是為了讓$\\textbf{單一資料}$Gradient計算時會比較漂亮，所以乘了一個$\\frac{1}{2}$。當初在寫Backward時一直不懂矩陣到底要怎麼乘，但後來發現只要想清楚上面的公式，再對照好矩陣維度就可以了！ Training再來來寫一個簡單的Training Function：123456789101112131415def train(self, X, y, epochs=1000, lr=0.01, verbose=10): &quot;&quot;&quot; Parameters: X (numpy.ndarray): Input matrix of shape (batch_size, n_input). y (numpy.ndarray): Target matrix of shape (batch_size, n_output). epochs (int): Number of epochs. lr (float): Learning rate. verbose (int): Frequency of printing the loss &quot;&quot;&quot; for epoch in range(epochs): pred = self.forward(X) loss = self.MSE(y, pred) self.backward(X, y, pred, lr) if epoch % verbose == 0: print(f&quot;Epoch {epoch}, Loss: {loss:.6f}&quot;) Conclusion這裡我以Regression為例，但實際上只要將Loss Function、Activation Function換掉，就可以應用在其他任務上。以下是一些常見的任務： Tasks 輸出層 Activation Function Loss Function 回歸（Regression） Identity Function Mean Squared Error 二元分類（Binary Classification） Sigmoid Binary Cross-Entropy 多類分類（Multi-Class Classification） Softmax Categorical Cross-Entropy 再做一些微分時小小的修改就可以了。但我認為重點在於理解Backpropagation中使用Chain Rule的技巧，還有計算Partial Derivative的一些細節。","link":"/2025/02/17/backpropagation/"},{"title":"Roofline Model for Performance Analysis","text":"A note about Roofline Model. Definition FLOPs : Number of Floating Point Operations. A Multiply-Add operation(MAC) is counted as 2 FLOPs. FLOPS : Floating Point Operations Per Second. $$ \\begin{aligned} \\text{FLOPS} &= \\frac{\\text{FLOPs}}{\\text{Second}} \\\\ \\end{aligned} $$ OPs : Number of Operations. (Not necessarily Floating Point Operations) OPS : Number of Operations Per Second. $$ \\begin{aligned} \\text{OPS} &= \\frac{\\text{OPs}}{\\text{Second}} \\\\ \\end{aligned} $$ Memory Bandwidth : The rate at which data can be read from or written to memory (Bytes per second). Arithmetic Intensity : The ratio of Total OPs performed to the Total Bytes moved. $$ \\begin{aligned} \\text{I} &= \\frac{C \\text{(Computations, OPs)}}{M \\text{(Memory Access, Bytes)}} \\\\ \\end{aligned} $$ Process Of DNN Inference on HardwareStep 1 (Memory Access): Moving Input/Weghts from memory to CPU/GPU.Step 2 (Compute): Perform the operations(e.g. Linear, Convolution…).Step 3 (Memory Access): Writing the computed activation back to memory.The process can be visualized as below: Therefore, evaluating performance requires simultaneous consideration of memory bandwidth and processing unit capabilities. If a layer involes extensive computation but minimal memory access, it is termed a computation bottleneck. Conversely, if a layer requires frequent memory access but minimal computation, it is termed a memory bottleneck. We can clearly distinguish between these two scenarios according to the Roofline Model. Roofline Model Plot the Roofline ModelFirstly, we need to determine the Peak Computational Performance (operations per second, OPS) $\\pi$ and Peak Memory Bandwidth(bytes per second) $\\beta$ specific to the target hardware device. Create a graph with performance (OPS) on the y-axis and arithmetic intensity (OPs/byte) on the x-axis: Draw a horizontal line equal to the peak computational performance, representing the maximum achievable performance by hardware. Draw a diagonal line from the original point with a slope equal to the peak memory bandwidth, representing the maximum memory bandwidth avaible on the system. After plotting these two lines, we can get Roofline Model as below: Analyze Performance for LayersFor each layer in LLMs, we can calculate its arithmetic intensity(OPs/byte) by dividing the required operations(OPs) by the amount of data transferred(bytes). According to the Roofline Model just plotted, the theoretical maximum performance for each layer is determined by the position on the graph corresponding to the arithmetic intensity of the layer. It allows us to ascertain whether this layer is compute-bound or memory-bound: If the layer’s arithmetic intensity is below the turning point, it means that the computational workload per memory access is low. Even saturating the memory bandwidth, it does not utilize the full computational capability of the hardware. In this case, the layer is constrained by memory access, and it is termed memory-bound. If the layer is memory-bound, we can optimize the performance by quantization, kernel fusion or increasing the batch size(decrease the number of memory access). Conversely, if the layer’s arithmetic intensity is above the turning point, it means that the computational workload per memory access is high. It implies that the layer requires only a small amount of memory access to consume a significant amount of computational capability, and it is termed compute-bound. Estimated Execution TimeThe execution time is the number of operations divided by performance $$ \\begin{aligned} \\text{T} &= \\frac{C}{P} = \\frac{C}{\\min(\\pi, \\beta \\cdot I)} \\\\ \\end{aligned} $$ Memory Bound: $$ \\begin{aligned} \\text{T} &= \\frac{C}{\\beta \\cdot I} = \\frac{C}{\\beta \\cdot \\frac{C}{M}} = \\frac{M}{\\beta}\\\\ \\end{aligned} $$ Compute Bound: $$ \\begin{aligned} \\text{T} &= \\frac{C}{\\pi} \\\\ \\end{aligned} $$ Example Usage of Roofline ModelHere, I provide an example of using the Roofline Model to analyze the performance of a specific layer in a neural network. Suppose we have some operations with #FLOPs and #Memory Access as below: Operation #FLOPs (M) #Memory Access (MB) op1 57.8M 25MB op2 51.4M 2.3MB op3 925M 25MB op4 236M 3.7MB op5 172M 1.3MB op6 231M 1.3MB Reduce #Memory Access when it’s Memory Bound (op1 vs op2) Reducing #FLOPs results no speed up. (refer op1 and op3) Higher computational capability(higher $\\pi$) does not improve performance. Operation #FLOPs (M) #Memory Access (MB) Operational Intensity Attainable Performance (GFLOPS) Theoretical Inference Time op1 57.8M 24.5MB 2.4 226.5 255 op2 51.4M 2.3MB 22.5 2156.7 23.8 op3 925M 24.5MB 37.7 3622.6 255 Reduce #FLOPs when it’s Compute Bound (op5 vs op6) Reducing #Memory Access results no speed up. (refer op4 and op6) Operation #FLOPs (M) #Memory Access (MB) Operational Intensity Attainable Performance (GFLOPS) Theoretical Inference Time op4 236M 3.7MB 64 4608 51.2 op5 172M 1.3MB 132 4608 37.3 op6 231M 1.3MB 174 4608 50.3 From these two scenarios, we can observe that smaller memory access or smaller FLOPs will not necessarily lead to better performance. ConclusionDuring the user’s interative with LLMs, the prefill stage excutes only once, while the decode stage is repeatedly performed to progressively generate the output. Moreover, when calculating the arithmetic intensity of each layer in LLMs, we will observe that the Prefill stage is almost entirely compute-bound, while the Decode stage is memory-bound. Therefore, optimizing for the memory bound characteristics of the decode stage becomes crucial for enhancing the inference performance of LLMs. Reference LLM Inference Unveiled: Survey and Roofline Model Insights Course slides of Edge AI, NYCU.","link":"/2025/03/09/roofline/"},{"title":"TIOJ 2070","text":"Solution of TIOJ 2070 - Special Judge。 Description給你由小寫英文字母組成的字串$X$，請判斷是不是每一個前$K$個英文字母的子集都是$X$的子序列。 Observation 1「前$k$個英文字母的『子集』的排列」都是子序列，其實就相當於要你判斷「是否前$k$個英文字母的的排列」都是子序列（abc的所有排列都是，那a, b, c, ab, ac, bc, abc也當然都會是）。 Observation 2範圍：$K \\le 20$ $\\rightarrow$位元DP！ Solution定義dp[S] = 當狀態為S時，字串所需的最短長度能夠使得滿足題目的要求，當dp[S]為n+1時，表示狀態S無法滿足。另外定義pos[i][j]表示在位置j往右出現字元i最近的位置，-1表示右邊找不到i。轉移式：$dp[i + (1 &lt;&lt; j)] = max(dp[i + (1 &lt;&lt; j)], pos[j][dp[i]]);$轉移過程中只要有出現了任何一個pos = -1就輸出$No$，否則就輸出$Yes$。 Code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;bits/stdc++.h&gt;#define int long long#define pb push_back#define all(x) (x).begin(), (x).end()#define fastio cin.tie(0); ios_base::sync_with_stdio(false);#define INF 1e15+9using namespace std;const int maxn = 2e5+5;const int MOD = 1e9+7; // 998244353;typedef pair&lt;int,int&gt; P;void solve(){ int k; string s; cin &gt;&gt; k &gt;&gt; s; char myChar[1005] = {}; for(int i=1;i&lt;=s.size();i++) myChar[i] = s[i-1]; int pos[50][1001]; for(int i=0;i&lt;k;i++){ int curPos = -1; for(int j=s.size();j&gt;=0;j--){ pos[i][j] = curPos; if(myChar[j] - 'a' == i) curPos = j; } } int S = (1 &lt;&lt; k); int dp[S+5] = {}; // dp[s] = 狀態s時, 會往右延伸到哪裡, INF表沒辦法 for(int i=0;i&lt;S;i++){ for(int j=0;j&lt;k;j++){ // 看看哪些沒在狀態i裡面 if((i &gt;&gt; j) % 2 == 0){ int nearest = pos[j][dp[i]]; if(nearest == -1){ cout &lt;&lt; &quot;No&quot; &lt;&lt; '\\n'; return; } dp[i + (1 &lt;&lt; j)] = max(dp[i + (1 &lt;&lt; j)], pos[j][dp[i]]); } } } cout &lt;&lt; &quot;Yes&quot; &lt;&lt; '\\n';}signed main(){fastio int T = 1; cin &gt;&gt; T; while(T--){ solve(); }} Bonus這一題的前身是「構造一個最短字串滿足此性質」，此題是一個co-NP-complete的問題目前能找到的最短且有系統的構造方法如下：12345678for(int i=1;i&lt;=k * k - 2 * k + 4;i++){ if(i &lt;= k) cout &lt;&lt; (char)('a' + i - 1); else if(i &gt; k &amp;&amp; (i - 2) % (k - 1) == 0) cout &lt;&lt; 'a'; else cout &lt;&lt; (char)((int)floor((i - 1) * (k - 2) / (k - 1)) % (k - 1) + 2 + 'a' - 1);}長度：$k^2 - 2k + 4$","link":"/2024/08/24/tioj2070/"},{"title":"我的第一篇貼文","text":"這是一篇測試用的文章。 首先先來測試一些基本的文字嗨，你好。I am Chung Pang Chun, and the handle I often use is bonginn.1 + 1 = 2 測試數學式$x^n + y^n = z^n$ \\begin{equation} \\begin{aligned} \\frac{-b + \\sqrt{b^2 - 4ac}}{2a} \\end{aligned}\\end{equation} 測試code12345678#include &lt;iostream&gt;using namespace std;int main(){ int a, b; cin &gt;&gt; a &gt;&gt; b; cout &lt;&lt; a + b;} 測試圖片 測試連結codeforces","link":"/2024/08/24/test/"}],"tags":[{"name":"code","slug":"code","link":"/tags/code/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"AI Efficiency","slug":"AI-Efficiency","link":"/tags/AI-Efficiency/"}],"categories":[{"name":"Note","slug":"Note","link":"/categories/Note/"},{"name":"Solution","slug":"Solution","link":"/categories/Solution/"}],"pages":[{"title":"About Me","text":"My name is Pang-Chun, Chung, and I am currently a junior at Department of Compuetr Science, National Yang Ming Chiao Tung University (NYCU). I’m interested in competitive programming, machine learning, and deep learning. I hope I can share what I have learned with my future self through this blog.If you have any questions regarding this blog posts, feel free to contact me via this email!","link":"/about/index.html"}]}